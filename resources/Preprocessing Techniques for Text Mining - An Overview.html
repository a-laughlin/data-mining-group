<html><head>
    <base href="https://webcache.googleusercontent.com/search?q=cache:55-8IWXlRB0J:https://www.ijcscn.com/Documents/Volumes/vol5issue1/ijcscn2015050102.pdf+&cd=1&hl=en&ct=clnk&gl=us"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><style>body{margin-left:0;margin-right:0;margin-top:0}#bN015htcoyT__google-cache-hdr{background:#f5f5f5;font:13px arial,sans-serif;text-align:left;color:#202020;border:0;margin:0;border-bottom:1px solid #cecece;line-height:16px;padding:16px 28px 24px 28px}#bN015htcoyT__google-cache-hdr *{display:inline;font:inherit;text-align:inherit;color:inherit;line-height:inherit;background:none;border:0;margin:0;padding:0;letter-spacing:0}#bN015htcoyT__google-cache-hdr a{text-decoration:none;color:#1a0dab}#bN015htcoyT__google-cache-hdr a:hover{text-decoration:underline}#bN015htcoyT__google-cache-hdr a:visited{color:#609}#bN015htcoyT__google-cache-hdr div{display:block;margin-top:4px}#bN015htcoyT__google-cache-hdr b{font-weight:bold;display:inline-block;direction:ltr}</style>
    <link rel="icon" href="data:image/x-icon;base64,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">
    <script id="savepage-contentloaders" type="application/javascript">
      "use strict"
      savepage_ContentLoaders();
      function savepage_ContentLoaders()
      {
        var resourceMimeType = new Array();
        var resourceBase64Data = new Array();
        var resourceBlobUrl = new Array();
        window.addEventListener("DOMContentLoaded",
        function(event)
        {
          savepage_ShadowLoader(5);
          savepage_ResourceLoader(5);
          document.getElementById('savepage-contentloaders').remove();
        },false);
        function savepage_ShadowLoader(c){createShadowDOMs(0,document.documentElement);function createShadowDOMs(a,b){var i;if(b.localName=="iframe"||b.localName=="frame"){if(a<c){try{if(b.contentDocument.documentElement!=null){createShadowDOMs(a+1,b.contentDocument.documentElement)}}catch(e){}}}else{if(b.children.length>=1&&b.children[0].localName=="template"&&b.children[0].hasAttribute("data-savepage-shadowroot")){b.attachShadow({mode:"open"}).appendChild(b.children[0].content);b.removeChild(b.children[0]);for(i=0;i<b.shadowRoot.children.length;i++)if(b.shadowRoot.children[i]!=null)createShadowDOMs(a,b.shadowRoot.children[i])}for(i=0;i<b.children.length;i++)if(b.children[i]!=null)createShadowDOMs(a,b.children[i])}}}
        function savepage_ResourceLoader(f){createBlobURLs();replaceReferences(0,document.documentElement);function createBlobURLs(){var i,j,binaryString,blobData;var a=new Array();for(i=0;i<resourceMimeType.length;i++){if(typeof resourceMimeType[i]!="undefined"){binaryString=atob(resourceBase64Data[i]);resourceBase64Data[i]="";a.length=0;for(j=0;j<binaryString.length;j++){a[j]=binaryString.charCodeAt(j)}blobData=new Blob([new Uint8Array(a)],{type:resourceMimeType[i]});resourceMimeType[i]="";resourceBlobUrl[i]=window.URL.createObjectURL(blobData)}}}function replaceReferences(a,b){var i,regex1,regex2,csstext,blobData;regex1=/url\(\s*((?:"[^"]+")|(?:'[^']+')|(?:[^\s)]+))\s*\)/gi;regex2=/data:[^;]*;resource=(\d+);base64,/i;if(b.hasAttribute("style")){csstext=b.style.cssText;b.style.cssText=csstext.replace(regex1,replaceCSSRef)}if(b.localName=="style"){csstext=b.textContent;b.textContent=csstext.replace(regex1,replaceCSSRef)}else if(b.localName=="link"&&(b.rel.toLowerCase()=="icon"||b.rel.toLowerCase()=="shortcut icon")){if(b.href!="")b.href=b.href.replace(regex2,replaceRef)}else if(b.localName=="body"){if(b.background!="")b.background=b.background.replace(regex2,replaceRef)}else if(b.localName=="img"){if(b.src!="")b.src=b.src.replace(regex2,replaceRef)}else if(b.localName=="input"&&b.type.toLowerCase()=="image"){if(b.src!="")b.src=b.src.replace(regex2,replaceRef)}else if(b.localName=="audio"){if(b.src!=""){b.src=b.src.replace(regex2,replaceRef);b.load()}}else if(b.localName=="video"){if(b.src!=""){b.src=b.src.replace(regex2,replaceRef);b.load()}if(b.poster!="")b.poster=b.poster.replace(regex2,replaceRef)}else if(b.localName=="source"){if(b.src!=""){b.src=b.src.replace(regex2,replaceRef);b.parentElement.load()}}else if(b.localName=="track"){if(b.src!="")b.src=b.src.replace(regex2,replaceRef)}else if(b.localName=="object"){if(b.data!="")b.data=b.data.replace(regex2,replaceRef)}else if(b.localName=="embed"){if(b.src!="")b.src=b.src.replace(regex2,replaceRef)}if(b.localName=="iframe"||b.localName=="frame"){if(a<f){if(b.hasAttribute("data-savepage-sameorigin")){blobData=new Blob([decodeURIComponent(b.src.substr(29))],{type:"text/html;charset=utf-8"});b.onload=function(){try{if(b.contentDocument.documentElement!=null){replaceReferences(a+1,b.contentDocument.documentElement)}}catch(e){}};b.src=window.URL.createObjectURL(blobData)}}}else{if(b.shadowRoot!=null){for(i=0;i<b.shadowRoot.children.length;i++)if(b.shadowRoot.children[i]!=null)replaceReferences(a,b.shadowRoot.children[i])}for(i=0;i<b.children.length;i++)if(b.children[i]!=null)replaceReferences(a,b.children[i])}}function replaceCSSRef(a,b,c,d){var e=new Array();e=b.match(/data:[^;]*;resource=(\d+);base64,/i);if(e!=null)return"url("+resourceBlobUrl[+e[1]]+")";else return a}function replaceRef(a,b,c,d){return resourceBlobUrl[+b]}}
      }
    </script>
    <meta name="savepage-url" content="https://webcache.googleusercontent.com/search?q=cache:55-8IWXlRB0J:https://www.ijcscn.com/Documents/Volumes/vol5issue1/ijcscn2015050102.pdf+&cd=1&hl=en&ct=clnk&gl=us">
    <meta name="savepage-title" content="Preprocessing Techniques for Text Mining - An Overview">
    <meta name="savepage-date" content="Thu Jun 20 2019 13:20:45 GMT-0600 (Mountain Daylight Time)">
    <meta name="savepage-state" content="Standard Items; Used resource loader; Retained cross-origin frames; Removed unsaved URLs; Max frame depth = 5; Max resource size = 50MB; Max resource time = 10s;">
    <meta name="savepage-version" content="15.0">
    <meta name="savepage-comments" content="">
    <meta name="savepage-resourceloader" content=""></head><body bgcolor="#ffffff" vlink="blue" link="blue"><div id="bN015htcoyT__google-cache-hdr"><div><span>This is the html version of the file <a href="https://www.ijcscn.com/Documents/Volumes/vol5issue1/ijcscn2015050102.pdf">https://www.ijcscn.com/Documents/Volumes/vol5issue1/ijcscn2015050102.pdf</a>. Google automatically generates html versions of documents as we crawl the web.</span></div><span style="display:inline-block;margin-top:8px;color:#717171"><span>Tip: To quickly find your search term on this page, press <b>Ctrl+F</b> or <b>⌘-F</b> (Mac) and use the find bar.</span></span></div><div style="position:relative;">

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="Author" content="ILAMATHI">
<meta name="CreationDate" content="D:20150217084113Z">
<meta name="Creator" content="Microsoft® Office Word 2007">
<meta name="ModDate" content="D:20150217141515+05'30'">
<meta name="Producer" content="Microsoft® Office Word 2007">
<title>Preprocessing Techniques for Text Mining - An Overview</title>

<table border="0" width="100%"><tbody><tr><td bgcolor="eeeeee" align="right"><font face="arial,sans-serif"><a name="1"><b>Page 1</b></a></font></td></tr></tbody></table><font size="3" face="Times"><span style="font-size:19px;font-family:Times">
<div style="position:absolute;top:288;left:200"><nobr><b>Preprocessing Techniques for Text Mining - An Overview</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:16px;font-family:Times">
<div style="position:absolute;top:330;left:285"><nobr>Dr. S. Vijayarani</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:10px;font-family:Times">
<div style="position:absolute;top:326;left:408"><nobr><i>1</i></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:16px;font-family:Times">
<div style="position:absolute;top:330;left:414"><nobr>, Ms. J. Ilamathi</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:10px;font-family:Times">
<div style="position:absolute;top:326;left:531"><nobr><i>2</i></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:16px;font-family:Times">
<div style="position:absolute;top:330;left:537"><nobr>, Ms. Nithya</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:10px;font-family:Times">
<div style="position:absolute;top:326;left:627"><nobr><i>3</i></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:16px;font-family:Times">
<div style="position:absolute;top:369;left:280"><nobr><i>Assistant Professor</i></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:10px;font-family:Times">
<div style="position:absolute;top:365;left:419"><nobr><i>1</i></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:16px;font-family:Times">
<div style="position:absolute;top:369;left:425"><nobr><i>, M. Phil Research Scholar</i></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:10px;font-family:Times">
<div style="position:absolute;top:365;left:620"><nobr><i>2, 3</i></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:16px;font-family:Times">
<div style="position:absolute;top:408;left:168"><nobr><i>Department of Computer Science, School of Computer Science and Engineering,</i></nobr></div>
<div style="position:absolute;top:447;left:250"><nobr><i>Bharathiar University, Coimbatore, Tamilnadu, India</i></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:10px;font-family:Times">
<div style="position:absolute;top:442;left:638"><nobr><i>1, 2, 3</i></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:16px;font-family:Times">
<div style="position:absolute;top:524;left:236"><nobr><b>Abstract</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:562;left:108"><nobr><i>Data mining is used for finding the useful</i></nobr></div>
<div style="position:absolute;top:579;left:108"><nobr><i>information from the large amount of data.  Data</i></nobr></div>
<div style="position:absolute;top:597;left:108"><nobr><i>mining techniques are used to implement and solve</i></nobr></div>
<div style="position:absolute;top:614;left:108"><nobr><i>different types of research problems.  The research</i></nobr></div>
<div style="position:absolute;top:631;left:108"><nobr><i>related areas in data mining are text mining, web</i></nobr></div>
<div style="position:absolute;top:648;left:108"><nobr><i>mining, image mining, sequential pattern mining,</i></nobr></div>
<div style="position:absolute;top:666;left:108"><nobr><i>spatial mining, medical mining, multimedia mining,</i></nobr></div>
<div style="position:absolute;top:683;left:108"><nobr><i>structure mining and graph mining.  This paper</i></nobr></div>
<div style="position:absolute;top:700;left:108"><nobr><i>discussed about the text mining and its preprocessing</i></nobr></div>
<div style="position:absolute;top:717;left:108"><nobr><i>techniques.  Text mining is the process of mining the</i></nobr></div>
<div style="position:absolute;top:735;left:108"><nobr><i>useful information from the text documents.  It is also</i></nobr></div>
<div style="position:absolute;top:752;left:108"><nobr><i>called knowledge discovery in text (KDT) or</i></nobr></div>
<div style="position:absolute;top:769;left:108"><nobr><i>knowledge of intelligent text analysis.  Text mining is</i></nobr></div>
<div style="position:absolute;top:786;left:108"><nobr><i>a technique which extracts information from both</i></nobr></div>
<div style="position:absolute;top:804;left:108"><nobr><i>structured and unstructured data and also finding</i></nobr></div>
<div style="position:absolute;top:821;left:108"><nobr><i>patterns.  Text mining techniques are used in various</i></nobr></div>
<div style="position:absolute;top:838;left:108"><nobr><i>types of research domains like natural language</i></nobr></div>
<div style="position:absolute;top:855;left:108"><nobr><i>processing, information retrieval, text classification</i></nobr></div>
<div style="position:absolute;top:873;left:108"><nobr><i>and text clustering.</i></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:16px;font-family:Times">
<div style="position:absolute;top:905;left:108"><nobr><b>Keywords</b>: <font style="font-size:13px">Text mining, Stemming, Stop words</font></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:925;left:108"><nobr>elimination, TF/IDF algorithms, Word Net, Word</nobr></div>
<div style="position:absolute;top:943;left:108"><nobr>Disambiguation.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:16px;font-family:Times">
<div style="position:absolute;top:976;left:108"><nobr><b>1.Introduction</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:1014;left:108"><nobr>Text mining is the process of seeking or extracting</nobr></div>
<div style="position:absolute;top:1033;left:108"><nobr>the useful information from the textual data. It is an</nobr></div>
<div style="position:absolute;top:1054;left:108"><nobr>exciting research area as it tries to discover</nobr></div>
<div style="position:absolute;top:1073;left:108"><nobr>knowledge from unstructured texts. It is also known</nobr></div>
<div style="position:absolute;top:1093;left:108"><nobr>as Text Data Mining (TDM) and knowledge</nobr></div>
<div style="position:absolute;top:1113;left:108"><nobr>Discovery in Textual Databases (KDT). KDT plays</nobr></div>
<div style="position:absolute;top:1133;left:108"><nobr>an increasingly significant role in emerging</nobr></div>
<div style="position:absolute;top:1153;left:108"><nobr>applications, such as Text Understanding. Text</nobr></div>
<div style="position:absolute;top:1172;left:108"><nobr>mining process is same as data mining, except, the</nobr></div>
<div style="position:absolute;top:1192;left:108"><nobr>data mining tools are designed to handle structured</nobr></div>
<div style="position:absolute;top:1212;left:108"><nobr>data whereas text mining can able to handle </nobr></div>
<div style="position:absolute;top:563;left:486"><nobr>unstructured or semi-structured data sets such as</nobr></div>
<div style="position:absolute;top:583;left:486"><nobr>emails HTML files and full text documents etc. [1].</nobr></div>
<div style="position:absolute;top:603;left:486"><nobr>Text Mining is used for finding the new, previously</nobr></div>
<div style="position:absolute;top:622;left:486"><nobr>unidentified information from different written</nobr></div>
<div style="position:absolute;top:642;left:486"><nobr>resources.</nobr></div>
<div style="position:absolute;top:682;left:486"><nobr>Structured data is data that resides in a fixed field</nobr></div>
<div style="position:absolute;top:702;left:486"><nobr>within a record or file.  This data is contained in</nobr></div>
<div style="position:absolute;top:721;left:486"><nobr>relational database and spreadsheets. The</nobr></div>
<div style="position:absolute;top:742;left:486"><nobr>unstructured data usually refers to information that</nobr></div>
<div style="position:absolute;top:761;left:486"><nobr>does not reside in a traditional row-column database</nobr></div>
<div style="position:absolute;top:781;left:486"><nobr>and it is the opposite of structured data. Semi-</nobr></div>
<div style="position:absolute;top:801;left:486"><nobr>Structured data is the data that is neither raw data, nor</nobr></div>
<div style="position:absolute;top:821;left:486"><nobr>typed data in a conventional database system. Text</nobr></div>
<div style="position:absolute;top:841;left:486"><nobr>mining is a new area of computer science research</nobr></div>
<div style="position:absolute;top:860;left:486"><nobr>that tries to solve the issues that occur in the area of</nobr></div>
<div style="position:absolute;top:880;left:486"><nobr>data mining, machine learning, information</nobr></div>
<div style="position:absolute;top:900;left:486"><nobr>extraction, natural language processing, information</nobr></div>
<div style="position:absolute;top:920;left:486"><nobr>retrieval, knowledge management and classification. </nobr></div>
<div style="position:absolute;top:940;left:486"><nobr>Figure 1 gives the overview of text mining process.</nobr></div>
<div style="position:absolute;top:1193;left:552"><nobr><b>Figure 1. Text Mining Process</b></nobr></div>
</span></font>
<font size="3" color="#800000" face="Times"><span style="font-size:13px;font-family:Times;color:#800000">
<div style="position:absolute;top:211;left:82"><nobr><i><b>Dr.S.Vijayarani et al , International Journal of Computer Science &amp; Communication Networks,Vol 5(1),7-16</b></i></nobr></div>
<div style="position:absolute;top:1307;left:799"><nobr><i><b>7</b></i></nobr></div>
</span></font>
<font size="3" color="#ab4000" face="Times"><span style="font-size:13px;font-family:Times;color:#ab4000">
<div style="position:absolute;top:189;left:763"><nobr><b>ISSN:2249-5789</b></nobr></div>
</span></font>

<div style="position:absolute;top:1363;left:0"><hr><table border="0" width="100%"><tbody><tr><td bgcolor="eeeeee" align="right"><font face="arial,sans-serif"><a name="2"><b>Page 2</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:1494;left:108"><nobr>The remaining portion of the paper is organized as</nobr></div>
<div style="position:absolute;top:1514;left:108"><nobr>follows. Section 2 gives the literature review. Section</nobr></div>
<div style="position:absolute;top:1534;left:108"><nobr>3 describes the text mining preprocessing methods.</nobr></div>
<div style="position:absolute;top:1554;left:108"><nobr>Stemming algorithms for classification are discussed</nobr></div>
<div style="position:absolute;top:1574;left:108"><nobr>in Section 4. Conclusion is given in Section 5. </nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:16px;font-family:Times">
<div style="position:absolute;top:1610;left:108"><nobr><b>1.1 Applications of Text Mining</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:1632;left:108"><nobr><b>Information Retrieval</b></nobr></div>
<div style="position:absolute;top:1652;left:108"><nobr>Information retrieval (IR) concept has been</nobr></div>
<div style="position:absolute;top:1669;left:108"><nobr>developed in relation with database systems for many</nobr></div>
<div style="position:absolute;top:1686;left:108"><nobr>years. Information retrieval is the association and</nobr></div>
<div style="position:absolute;top:1704;left:108"><nobr>retrieval of information from a large number of text-</nobr></div>
<div style="position:absolute;top:1721;left:108"><nobr>based documents. The information retrieval and</nobr></div>
<div style="position:absolute;top:1738;left:108"><nobr>database systems, each handle various kinds of data;</nobr></div>
<div style="position:absolute;top:1755;left:108"><nobr>some database system problems are usually not</nobr></div>
<div style="position:absolute;top:1773;left:108"><nobr>present in information retrieval systems, such as</nobr></div>
<div style="position:absolute;top:1790;left:108"><nobr>concurrency control, recovery, transaction</nobr></div>
<div style="position:absolute;top:1807;left:108"><nobr>management, and update. Also, some common</nobr></div>
<div style="position:absolute;top:1824;left:108"><nobr>information retrieval problems are usually not</nobr></div>
<div style="position:absolute;top:1842;left:108"><nobr>encountered in conventional database systems, such</nobr></div>
<div style="position:absolute;top:1859;left:108"><nobr>as unstructured documents, estimated search based on</nobr></div>
<div style="position:absolute;top:1876;left:108"><nobr>keywords, and the concept of relevance. Due to the</nobr></div>
<div style="position:absolute;top:1894;left:108"><nobr>huge quantity of text information, information</nobr></div>
<div style="position:absolute;top:1911;left:108"><nobr>retrieval has found many applications. There exist</nobr></div>
<div style="position:absolute;top:1928;left:108"><nobr>many information retrieval systems, such as on-line</nobr></div>
<div style="position:absolute;top:1945;left:108"><nobr>library catalog systems, on-line document</nobr></div>
<div style="position:absolute;top:1963;left:108"><nobr>management systems, and the more recently</nobr></div>
<div style="position:absolute;top:1980;left:108"><nobr>developed Web search engines [1].</nobr></div>
<div style="position:absolute;top:2015;left:108"><nobr><b>Information Extraction</b></nobr></div>
<div style="position:absolute;top:2034;left:113"><nobr>The information extraction method identifies key</nobr></div>
<div style="position:absolute;top:2054;left:108"><nobr>words and relationships within the text. It does this</nobr></div>
<div style="position:absolute;top:2074;left:108"><nobr>by looking for predefined sequences in the text, a</nobr></div>
<div style="position:absolute;top:2094;left:108"><nobr>process called pattern matching. The software infers</nobr></div>
<div style="position:absolute;top:2113;left:108"><nobr>the relationships between all the identified places,</nobr></div>
<div style="position:absolute;top:2133;left:108"><nobr>people, and time to give the user with meaningful</nobr></div>
<div style="position:absolute;top:2153;left:108"><nobr>information. This technology is very useful when</nobr></div>
<div style="position:absolute;top:2173;left:108"><nobr>dealing with large volumes of text. Traditional data</nobr></div>
<div style="position:absolute;top:2193;left:108"><nobr>mining assumes that the information being “mined”</nobr></div>
<div style="position:absolute;top:2213;left:108"><nobr>is already in the form of a relational database.</nobr></div>
<div style="position:absolute;top:2233;left:108"><nobr>Unfortunately, for many applications, electronic</nobr></div>
<div style="position:absolute;top:2252;left:108"><nobr>information is only available in the form of free</nobr></div>
<div style="position:absolute;top:2272;left:108"><nobr>natural language documents rather than structured</nobr></div>
<div style="position:absolute;top:2292;left:108"><nobr>databases [1]. This process is depicted in Figure 2.</nobr></div>
<div style="position:absolute;top:1650;left:533"><nobr><b>Figure 2. Process of Text Extraction</b></nobr></div>
<div style="position:absolute;top:1670;left:486"><nobr><b>Categorization</b></nobr></div>
<div style="position:absolute;top:1690;left:486"><nobr>Categorization involves identifying the main themes</nobr></div>
<div style="position:absolute;top:1707;left:486"><nobr>of a document by inserting the document into a pre-</nobr></div>
<div style="position:absolute;top:1724;left:486"><nobr>defined set of topics. When categorizing a document,</nobr></div>
<div style="position:absolute;top:1742;left:486"><nobr>a computer program will often treat the document as</nobr></div>
<div style="position:absolute;top:1756;left:486"><nobr>a “bag of words.” It does not try to process the actual</nobr></div>
<div style="position:absolute;top:1776;left:486"><nobr>information as information extraction does. Rather,</nobr></div>
<div style="position:absolute;top:1793;left:486"><nobr>the categorization only counts words that appear and,</nobr></div>
<div style="position:absolute;top:1811;left:486"><nobr>from the counts, identifies the main topics that the</nobr></div>
<div style="position:absolute;top:1828;left:486"><nobr>document covers. Categorization often relies on a</nobr></div>
<div style="position:absolute;top:1845;left:486"><nobr>glossary for which topics are predefined, and</nobr></div>
<div style="position:absolute;top:1862;left:486"><nobr>relationships are identified by looking for large</nobr></div>
<div style="position:absolute;top:1880;left:486"><nobr>terms, narrower terms, synonyms, and related terms</nobr></div>
<div style="position:absolute;top:1897;left:486"><nobr>[4].</nobr></div>
<div style="position:absolute;top:1938;left:486"><nobr><b>Natural Language Processing</b></nobr></div>
<div style="position:absolute;top:1958;left:486"><nobr>Natural Language Processing (NLP) is an area of</nobr></div>
<div style="position:absolute;top:1975;left:486"><nobr>research and application that explores how computers</nobr></div>
<div style="position:absolute;top:1992;left:486"><nobr>can be used to understand and manipulate natural</nobr></div>
<div style="position:absolute;top:2010;left:486"><nobr>language text. NLP researchers aim to collect</nobr></div>
<div style="position:absolute;top:2027;left:486"><nobr>knowledge on how human beings understand and use</nobr></div>
<div style="position:absolute;top:2044;left:486"><nobr>language so that fitting tools and techniques can be</nobr></div>
<div style="position:absolute;top:2061;left:486"><nobr>developed to make computer systems understand and</nobr></div>
<div style="position:absolute;top:2079;left:486"><nobr>manipulate natural languages to perform the</nobr></div>
<div style="position:absolute;top:2096;left:486"><nobr>preferred tasks [3]. </nobr></div>
<div style="position:absolute;top:2137;left:486"><nobr>The basics of NLP lie in a number of disciplines, viz.</nobr></div>
<div style="position:absolute;top:2154;left:486"><nobr>computer and information sciences, linguistics,</nobr></div>
<div style="position:absolute;top:2171;left:486"><nobr>mathematics, electrical and electronic engineering,</nobr></div>
<div style="position:absolute;top:2188;left:486"><nobr>artificial intelligence and robotics, psychology, etc.</nobr></div>
<div style="position:absolute;top:2206;left:486"><nobr>Applications of NLP include a number of fields of</nobr></div>
<div style="position:absolute;top:2223;left:486"><nobr>studies, such as machine translation, natural language</nobr></div>
<div style="position:absolute;top:2240;left:486"><nobr>text processing and summarization, user interfaces,</nobr></div>
<div style="position:absolute;top:2257;left:486"><nobr>multilingual and cross language information retrieval</nobr></div>
<div style="position:absolute;top:2275;left:486"><nobr>(CLIR), speech recognition, artificial intelligence and</nobr></div>
<div style="position:absolute;top:2292;left:486"><nobr>expert systems and so on[3]. </nobr></div>
</span></font>
<font size="3" color="#800000" face="Times"><span style="font-size:13px;font-family:Times;color:#800000">
<div style="position:absolute;top:1399;left:82"><nobr><i><b>Dr.S.Vijayarani et al , International Journal of Computer Science &amp; Communication Networks,Vol 5(1),7-16</b></i></nobr></div>
<div style="position:absolute;top:2495;left:799"><nobr><i><b>8</b></i></nobr></div>
</span></font>
<font size="3" color="#ab4000" face="Times"><span style="font-size:13px;font-family:Times;color:#ab4000">
<div style="position:absolute;top:1377;left:763"><nobr><b>ISSN:2249-5789</b></nobr></div>
</span></font>

<div style="position:absolute;top:2551;left:0"><hr><table border="0" width="100%"><tbody><tr><td bgcolor="eeeeee" align="right"><font face="arial,sans-serif"><a name="3"><b>Page 3</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size:16px;font-family:Times">
<div style="position:absolute;top:2663;left:108"><nobr><b>2. Literature Review</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:2698;left:108"><nobr>Anjali Ganesh Jivani [22] discussed that the purpose</nobr></div>
<div style="position:absolute;top:2715;left:108"><nobr>of stemming is to reduce different grammatical forms</nobr></div>
<div style="position:absolute;top:2733;left:108"><nobr>or word forms of a word like its noun, adjective,</nobr></div>
<div style="position:absolute;top:2750;left:108"><nobr>verb, adverb etc. The goal of stemming is to reduce</nobr></div>
<div style="position:absolute;top:2767;left:108"><nobr>inflectional forms and sometimes derivationally</nobr></div>
<div style="position:absolute;top:2784;left:108"><nobr>related forms of a word to a common base form. This</nobr></div>
<div style="position:absolute;top:2802;left:108"><nobr>paper discusses different methods of stemming and</nobr></div>
<div style="position:absolute;top:2819;left:108"><nobr>their comparisons in terms of usage, advantages as</nobr></div>
<div style="position:absolute;top:2836;left:108"><nobr>well as limitations. The basic difference between</nobr></div>
<div style="position:absolute;top:2853;left:108"><nobr>stemming and lemmatization is also discussed. </nobr></div>
<div style="position:absolute;top:2886;left:108"><nobr>Vishal Gupta et.al [23] has analyzed the stemmer‟s</nobr></div>
<div style="position:absolute;top:2903;left:108"><nobr>performance and effectiveness in applications such as</nobr></div>
<div style="position:absolute;top:2920;left:108"><nobr>spelling checker varies across languages. A typical</nobr></div>
<div style="position:absolute;top:2937;left:108"><nobr>simple stemmer algorithm involves removing</nobr></div>
<div style="position:absolute;top:2955;left:108"><nobr>suffixes using a list of frequent suffixes, while a more</nobr></div>
<div style="position:absolute;top:2972;left:108"><nobr>complex one would use morphological knowledge to</nobr></div>
<div style="position:absolute;top:2989;left:108"><nobr>derive a stem from the words. The paper gives a</nobr></div>
<div style="position:absolute;top:3006;left:108"><nobr>detailed outline of common stemming techniques and</nobr></div>
<div style="position:absolute;top:3024;left:108"><nobr>existing stemmers for Indian languages.</nobr></div>
<div style="position:absolute;top:3073;left:108"><nobr>K.K. Agbele [24] discussed the technique for</nobr></div>
<div style="position:absolute;top:3090;left:108"><nobr>developing pervasive computing applications that are</nobr></div>
<div style="position:absolute;top:3108;left:108"><nobr>flexible and adaptable for users. In this context,</nobr></div>
<div style="position:absolute;top:3125;left:108"><nobr>however, information retrieval (IR) is often defined</nobr></div>
<div style="position:absolute;top:3142;left:108"><nobr>in terms of location and delivery of documents to a</nobr></div>
<div style="position:absolute;top:3159;left:108"><nobr>user to satisfy their information need. In most cases,</nobr></div>
<div style="position:absolute;top:3177;left:108"><nobr>morphological variants of words have similar</nobr></div>
<div style="position:absolute;top:3194;left:108"><nobr>semantic interpretations and can be considered as</nobr></div>
<div style="position:absolute;top:3211;left:108"><nobr>equivalent for the purpose of IR applications. The</nobr></div>
<div style="position:absolute;top:3228;left:108"><nobr>algorithm Context-Aware Stemming (CAS) is</nobr></div>
<div style="position:absolute;top:3246;left:108"><nobr>proposed, which is a modified version of the</nobr></div>
<div style="position:absolute;top:3260;left:108"><nobr>extensively used Porter‟s stemmer. Considering only</nobr></div>
<div style="position:absolute;top:3280;left:108"><nobr>generated meaningful stemming words as the</nobr></div>
<div style="position:absolute;top:3297;left:108"><nobr>stemmer output, the results show that the modified</nobr></div>
<div style="position:absolute;top:3315;left:108"><nobr>algorithm significantly reduces the error rate of</nobr></div>
<div style="position:absolute;top:3329;left:108"><nobr>Porter‟s algorithm from 76.7% to 6.7% without</nobr></div>
<div style="position:absolute;top:3346;left:108"><nobr>compromising the efficacy of Porter‟s algorithm.</nobr></div>
<div style="position:absolute;top:3384;left:108"><nobr>Hassan Saif [25] has investigated whether removing</nobr></div>
<div style="position:absolute;top:3401;left:108"><nobr>stop words helps or hampers the effectiveness of</nobr></div>
<div style="position:absolute;top:3418;left:108"><nobr>Twitter sentiment classification methods. For this</nobr></div>
<div style="position:absolute;top:3435;left:108"><nobr>investigation he has applied, six different stop word</nobr></div>
<div style="position:absolute;top:3453;left:108"><nobr>identification methods to Twitter data from six</nobr></div>
<div style="position:absolute;top:3470;left:108"><nobr>different datasets and observe how removing stop</nobr></div>
<div style="position:absolute;top:3487;left:108"><nobr>words affects two well-known supervised sentiment</nobr></div>
<div style="position:absolute;top:3504;left:108"><nobr>classification methods. The result shows that using</nobr></div>
<div style="position:absolute;top:3522;left:108"><nobr>pre-compiled lists of stop words negatively impacts</nobr></div>
<div style="position:absolute;top:3539;left:108"><nobr>the performance of Twitter sentiment classification</nobr></div>
<div style="position:absolute;top:3556;left:108"><nobr>approaches. On the other hand, the dynamic</nobr></div>
<div style="position:absolute;top:3573;left:108"><nobr>generation of stopword lists, by removing those</nobr></div>
<div style="position:absolute;top:3591;left:108"><nobr>infrequent</nobr></div>
<div style="position:absolute;top:2699;left:486"><nobr>terms appearing only once in the corpus appears to be</nobr></div>
<div style="position:absolute;top:2716;left:486"><nobr>the optimal method for maintaining a high</nobr></div>
<div style="position:absolute;top:2733;left:486"><nobr>classification performance while reducing the data</nobr></div>
<div style="position:absolute;top:2750;left:486"><nobr>sparsity and substantially shrinking the feature space.</nobr></div>
<div style="position:absolute;top:2785;left:486"><nobr><b>3. Preprocessing methods</b></nobr></div>
<div style="position:absolute;top:2823;left:486"><nobr>Preprocessing method plays a very important role in</nobr></div>
<div style="position:absolute;top:2840;left:486"><nobr>text mining techniques and applications.  It is the first</nobr></div>
<div style="position:absolute;top:2857;left:486"><nobr>step in the text mining process.  In this paper, we</nobr></div>
<div style="position:absolute;top:2874;left:486"><nobr>discuss the three key steps of preprocessing namely,</nobr></div>
<div style="position:absolute;top:2892;left:486"><nobr>stop words removal, stemming and TF/IDF</nobr></div>
<div style="position:absolute;top:2909;left:486"><nobr>algorithms (Figure 3).</nobr></div>
<div style="position:absolute;top:3204;left:489"><nobr><b>Figure 3. Text Mining Pre-Processing Techniques</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:14px;font-family:Times">
<div style="position:absolute;top:3248;left:486"><nobr><b>A. Extraction</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:3269;left:540"><nobr>This method is used to tokenize the file</nobr></div>
<div style="position:absolute;top:3286;left:486"><nobr>content into individual word.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:14px;font-family:Times">
<div style="position:absolute;top:3322;left:486"><nobr><b>B. Stop Words Elimination </b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:3343;left:486"><nobr>Stop words are a division of natural language. The</nobr></div>
<div style="position:absolute;top:3360;left:486"><nobr>motive that stop-words should be removed from a</nobr></div>
<div style="position:absolute;top:3377;left:486"><nobr>text is that they make the text look heavier and less</nobr></div>
<div style="position:absolute;top:3394;left:486"><nobr>important for analysts. Removing stop words reduces</nobr></div>
<div style="position:absolute;top:3412;left:486"><nobr>the dimensionality of term space. The most common</nobr></div>
<div style="position:absolute;top:3429;left:486"><nobr>words in text documents are articles, prepositions,</nobr></div>
<div style="position:absolute;top:3446;left:486"><nobr>and pro-nouns, etc. that does not give the meaning of</nobr></div>
<div style="position:absolute;top:3463;left:486"><nobr>the documents. These words are treated as stop</nobr></div>
<div style="position:absolute;top:3481;left:486"><nobr>words. Example for stop words: the, in, a, an, with,</nobr></div>
<div style="position:absolute;top:3498;left:486"><nobr>etc. Stop words are removed from documents</nobr></div>
<div style="position:absolute;top:3515;left:486"><nobr>because those words are not measured as keywords in</nobr></div>
<div style="position:absolute;top:3533;left:486"><nobr>text mining applications [5].</nobr></div>
</span></font>
<font size="3" color="#800000" face="Times"><span style="font-size:13px;font-family:Times;color:#800000">
<div style="position:absolute;top:2587;left:82"><nobr><i><b>Dr.S.Vijayarani et al , International Journal of Computer Science &amp; Communication Networks,Vol 5(1),7-16</b></i></nobr></div>
<div style="position:absolute;top:3683;left:799"><nobr><i><b>9</b></i></nobr></div>
</span></font>
<font size="3" color="#ab4000" face="Times"><span style="font-size:13px;font-family:Times;color:#ab4000">
<div style="position:absolute;top:2565;left:763"><nobr><b>ISSN:2249-5789</b></nobr></div>
</span></font>

<div style="position:absolute;top:3739;left:0"><hr><table border="0" width="100%"><tbody><tr><td bgcolor="eeeeee" align="right"><font face="arial,sans-serif"><a name="4"><b>Page 4</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size:14px;font-family:Times">
<div style="position:absolute;top:3851;left:108"><nobr><b>C. Stop word removal methods</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:3872;left:108"><nobr>Four types of stop word removal methods are</nobr></div>
<div style="position:absolute;top:3889;left:108"><nobr>followed, the methods are used to remove stop words</nobr></div>
<div style="position:absolute;top:3907;left:108"><nobr>from the files [5].</nobr></div>
<div style="position:absolute;top:3924;left:108"><nobr><b>i. The Classic Method: </b>The classic method is based</nobr></div>
<div style="position:absolute;top:3941;left:108"><nobr>on removing stop words obtained from pre-compiled</nobr></div>
<div style="position:absolute;top:3958;left:108"><nobr>lists [7].  </nobr></div>
<div style="position:absolute;top:3993;left:108"><nobr><b>ii. Methods based on Zipf’s Law (Z-Methods): </b>In</nobr></div>
<div style="position:absolute;top:4010;left:108"><nobr>addition to the classic stop list, we use three stop</nobr></div>
<div style="position:absolute;top:4027;left:108"><nobr>word creation methods moved by Zipf‟s law,</nobr></div>
<div style="position:absolute;top:4045;left:108"><nobr>including: removing most frequent words (TF-High)</nobr></div>
<div style="position:absolute;top:4062;left:108"><nobr>and removing words that occur once, i.e. singleton</nobr></div>
<div style="position:absolute;top:4079;left:108"><nobr>words (TF1). We also consider removing words with</nobr></div>
<div style="position:absolute;top:4096;left:108"><nobr>low inverse document frequency (IDF) [7, 8]. </nobr></div>
<div style="position:absolute;top:4131;left:108"><nobr><b>iii. The Mutual Information Method (MI)</b></nobr></div>
<div style="position:absolute;top:4148;left:108"><nobr>The mutual information method (MI) is a supervised</nobr></div>
<div style="position:absolute;top:4165;left:108"><nobr>method that works by computing the mutual</nobr></div>
<div style="position:absolute;top:4183;left:108"><nobr>information between a given term and a document</nobr></div>
<div style="position:absolute;top:4200;left:108"><nobr>class (e.g., positive, negative), providing a suggestion</nobr></div>
<div style="position:absolute;top:4217;left:108"><nobr>of how much information the term can tell about a</nobr></div>
<div style="position:absolute;top:4234;left:108"><nobr>given class. Low mutual information suggests that</nobr></div>
<div style="position:absolute;top:4252;left:108"><nobr>the term has a low discrimination power and</nobr></div>
<div style="position:absolute;top:4269;left:108"><nobr>consequently it should be removed [7, 8]. </nobr></div>
<div style="position:absolute;top:4304;left:108"><nobr><b>iv. Term Based Random Sampling (TBRS)</b></nobr></div>
<div style="position:absolute;top:4321;left:108"><nobr>This method was first proposed by Lo et al. (2005) to</nobr></div>
<div style="position:absolute;top:4338;left:108"><nobr>manually detect the stop words from web documents.</nobr></div>
<div style="position:absolute;top:4355;left:108"><nobr>This method works by iterating over separate chunks</nobr></div>
<div style="position:absolute;top:4372;left:108"><nobr>of data which are randomly selected. It then ranks</nobr></div>
<div style="position:absolute;top:4390;left:108"><nobr>terms in each chunk based on their in format values</nobr></div>
<div style="position:absolute;top:4407;left:108"><nobr>using the Kullback-Leibler divergence measure as</nobr></div>
<div style="position:absolute;top:4424;left:108"><nobr>shown in Equation 1.</nobr></div>
<div style="position:absolute;top:4446;left:170"><nobr>d<font style="font-size:7px">x </font>(t) = P<font style="font-size:7px">x </font>(t).log<font style="font-size:7px">2</font></nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size:8px;font-family:Times">
<div style="position:absolute;top:4440;left:278"><nobr>Px (t)</nobr></div>
<div style="position:absolute;top:4456;left:280"><nobr>p (t)</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size:7px;font-family:Times">
<div style="position:absolute;top:4452;left:358"><nobr>(1)</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:4470;left:108"><nobr>Where Px (t) is the normalized term frequency of a</nobr></div>
<div style="position:absolute;top:4487;left:108"><nobr>term t within a mass x, and P(t) is the normalized</nobr></div>
<div style="position:absolute;top:4504;left:108"><nobr>term frequency of t in the entire collection. The final</nobr></div>
<div style="position:absolute;top:4522;left:108"><nobr>stop list is then constructed by taking the least</nobr></div>
<div style="position:absolute;top:4539;left:108"><nobr>informative terms in all chunks, removing all</nobr></div>
<div style="position:absolute;top:4556;left:108"><nobr>possible duplications [7].</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:14px;font-family:Times">
<div style="position:absolute;top:4591;left:108"><nobr><b>D. Stemming</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:4609;left:108"><nobr>This method is used to identify the root/stem of a</nobr></div>
<div style="position:absolute;top:4627;left:108"><nobr>word. For example, the words connect, connected,</nobr></div>
<div style="position:absolute;top:4644;left:108"><nobr>connecting, connections all can be stemmed to the</nobr></div>
<div style="position:absolute;top:4658;left:108"><nobr>word “<b>connect</b>” [6]. The purpose of this method is to</nobr></div>
<div style="position:absolute;top:4678;left:108"><nobr>remove various suffixes, to reduce the number of</nobr></div>
<div style="position:absolute;top:4696;left:108"><nobr>words, to have accurately matching stems, to save</nobr></div>
<div style="position:absolute;top:4713;left:108"><nobr>time and memory space. This is illustrated in Figure</nobr></div>
<div style="position:absolute;top:4730;left:108"><nobr>4.</nobr></div>
<div style="position:absolute;top:4074;left:559"><nobr><b>Figure 4. Stemming Process</b></nobr></div>
<div style="position:absolute;top:4093;left:486"><nobr>In stemming, translation of morphological forms of a</nobr></div>
<div style="position:absolute;top:4110;left:486"><nobr>word to its stem is done assuming each one is</nobr></div>
<div style="position:absolute;top:4128;left:486"><nobr>semantically related. There are two points are</nobr></div>
<div style="position:absolute;top:4145;left:486"><nobr>considered while using a stemmer:</nobr></div>
</span></font>
<font size="3" face="Symbol"><span style="font-size:13px;font-family:Symbol">
<div style="position:absolute;top:4159;left:529"><nobr>• <font face="Times">Words that do not have the same meaning</font></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:4181;left:539"><nobr>should be kept separate</nobr></div>
</span></font>
<font size="3" face="Symbol"><span style="font-size:13px;font-family:Symbol">
<div style="position:absolute;top:4194;left:529"><nobr>• <font face="Times">Morphological forms of a word are assumed</font></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:4216;left:539"><nobr>to have the same base meaning and hence it</nobr></div>
<div style="position:absolute;top:4233;left:539"><nobr>should be mapped to the same stem</nobr></div>
<div style="position:absolute;top:4251;left:486"><nobr>These two rules are good and sufficient in text</nobr></div>
<div style="position:absolute;top:4268;left:486"><nobr>mining or language processing applications.</nobr></div>
<div style="position:absolute;top:4285;left:486"><nobr>Stemming is usually considered as a recall-enhancing</nobr></div>
<div style="position:absolute;top:4303;left:486"><nobr>device. For languages with relatively simple</nobr></div>
<div style="position:absolute;top:4320;left:486"><nobr>morphology, the power of stemming is less than for</nobr></div>
<div style="position:absolute;top:4337;left:486"><nobr>those with a more complex morphology. Most of the</nobr></div>
<div style="position:absolute;top:4354;left:486"><nobr>stemming experiments done so far are in english and</nobr></div>
<div style="position:absolute;top:4372;left:486"><nobr>other west European languages.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:16px;font-family:Times">
<div style="position:absolute;top:4424;left:486"><nobr><b>4. Stemming Algorithms for Classification</b></nobr></div>
<div style="position:absolute;top:4445;left:486"><nobr><b>Process</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:4465;left:486"><nobr>Usually, stemming algorithms can be classified into</nobr></div>
<div style="position:absolute;top:4482;left:486"><nobr>three groups: truncating methods, statistical methods,</nobr></div>
<div style="position:absolute;top:4499;left:486"><nobr>and mixed methods [8]. Each of these groups has a</nobr></div>
<div style="position:absolute;top:4516;left:486"><nobr>typical way of finding the stems of the word variants.</nobr></div>
<div style="position:absolute;top:4534;left:486"><nobr>These methods and the algorithms discussed in this</nobr></div>
<div style="position:absolute;top:4554;left:486"><nobr>paper are shown in the Figure 5<font style="font-size:16px">.</font></nobr></div>
<div style="position:absolute;top:4760;left:547"><nobr><b>Figure 5. Stemming Algorithms</b></nobr></div>
</span></font>
<font size="3" color="#800000" face="Times"><span style="font-size:13px;font-family:Times;color:#800000">
<div style="position:absolute;top:3775;left:82"><nobr><i><b>Dr.S.Vijayarani et al , International Journal of Computer Science &amp; Communication Networks,Vol 5(1),7-16</b></i></nobr></div>
<div style="position:absolute;top:4871;left:790"><nobr><i><b>10</b></i></nobr></div>
</span></font>
<font size="3" color="#ab4000" face="Times"><span style="font-size:13px;font-family:Times;color:#ab4000">
<div style="position:absolute;top:3753;left:763"><nobr><b>ISSN:2249-5789</b></nobr></div>
</span></font>

<div style="position:absolute;top:4927;left:0"><hr><table border="0" width="100%"><tbody><tr><td bgcolor="eeeeee" align="right"><font face="arial,sans-serif"><a name="5"><b>Page 5</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size:14px;font-family:Times">
<div style="position:absolute;top:5039;left:108"><nobr><b>A. Truncating Methods (Affix Removal)</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:5060;left:108"><nobr>As the name obviously suggests these methods are</nobr></div>
<div style="position:absolute;top:5077;left:108"><nobr>related to removing the suffixes or prefixes</nobr></div>
<div style="position:absolute;top:5095;left:108"><nobr>(commonly known as affixes) of a word [8]. The</nobr></div>
<div style="position:absolute;top:5112;left:108"><nobr>most basic stemmer is the Truncate (n) stemmer</nobr></div>
<div style="position:absolute;top:5129;left:108"><nobr>which truncated a word at the nth symbol i.e. keep n</nobr></div>
<div style="position:absolute;top:5146;left:108"><nobr>letters and remove the rest. In this method words</nobr></div>
<div style="position:absolute;top:5164;left:108"><nobr>shorter than n are kept as it is. The probability of over</nobr></div>
<div style="position:absolute;top:5181;left:108"><nobr>stemming increases when the word length is small.</nobr></div>
<div style="position:absolute;top:5198;left:108"><nobr>Another simple approach was the S-stemmer – an</nobr></div>
<div style="position:absolute;top:5215;left:108"><nobr>algorithm conflating singular and plural forms of</nobr></div>
<div style="position:absolute;top:5233;left:108"><nobr>English nouns. This algorithm was proposed by</nobr></div>
<div style="position:absolute;top:5250;left:108"><nobr>Donna Harman. The algorithm has rules to remove</nobr></div>
<div style="position:absolute;top:5267;left:108"><nobr>suffixes in plurals so as to convert them to the</nobr></div>
<div style="position:absolute;top:5284;left:108"><nobr>singular forms [9].</nobr></div>
<div style="position:absolute;top:5319;left:108"><nobr><b>1. Lovins Stemmer</b></nobr></div>
<div style="position:absolute;top:5339;left:108"><nobr>This was the first trendy and effective stemmer</nobr></div>
<div style="position:absolute;top:5356;left:108"><nobr>proposed by Lovins in 1968. The Lovins stemmer</nobr></div>
<div style="position:absolute;top:5373;left:108"><nobr>removes the longest suffix from a word. Once the</nobr></div>
<div style="position:absolute;top:5391;left:108"><nobr>ending is removed, the word is recoded using a</nobr></div>
<div style="position:absolute;top:5408;left:108"><nobr>different table that makes various adjustments to</nobr></div>
<div style="position:absolute;top:5425;left:108"><nobr>convert these stems into valid words. It always</nobr></div>
<div style="position:absolute;top:5442;left:108"><nobr>removes a maximum of one suffix from a word, due</nobr></div>
<div style="position:absolute;top:5460;left:108"><nobr>to its nature as a single pass algorithm. The</nobr></div>
<div style="position:absolute;top:5477;left:108"><nobr>advantages of this algorithm are it is very fast and</nobr></div>
<div style="position:absolute;top:5494;left:108"><nobr>can handle the removal of double letters in words like</nobr></div>
<div style="position:absolute;top:5508;left:108"><nobr>„getting‟ being transformed to „get‟ and also handles</nobr></div>
<div style="position:absolute;top:5529;left:108"><nobr>many irregular plurals like – mouse and mice, index</nobr></div>
<div style="position:absolute;top:5546;left:108"><nobr>and indices etc. A drawback of the Lovins approach</nobr></div>
<div style="position:absolute;top:5563;left:108"><nobr>is it is time consuming one. Furthermore, many</nobr></div>
<div style="position:absolute;top:5580;left:108"><nobr>suffixes are not available in the table of endings. It is</nobr></div>
<div style="position:absolute;top:5598;left:108"><nobr>sometimes highly unreliable and frequently fails to</nobr></div>
<div style="position:absolute;top:5615;left:108"><nobr>form words from the stems or to match the stems of</nobr></div>
<div style="position:absolute;top:5632;left:108"><nobr>like-meaning words. </nobr></div>
<div style="position:absolute;top:5667;left:108"><nobr><b>2. Porters Stemmer</b></nobr></div>
<div style="position:absolute;top:5684;left:108"><nobr>Porters stemming algorithm [11, 12] is one of the</nobr></div>
<div style="position:absolute;top:5701;left:108"><nobr>most popular stemming algorithm proposed in 1980.</nobr></div>
<div style="position:absolute;top:5718;left:108"><nobr>Many modifications and enhancements have been</nobr></div>
<div style="position:absolute;top:5736;left:108"><nobr>made and suggested on the basic algorithm. It is</nobr></div>
<div style="position:absolute;top:5753;left:108"><nobr>based on the idea that the suffixes in the English</nobr></div>
<div style="position:absolute;top:5770;left:108"><nobr>language (approximately 1200) are mostly made up</nobr></div>
<div style="position:absolute;top:5787;left:108"><nobr>of grouping of smaller and simpler suffixes. It has</nobr></div>
<div style="position:absolute;top:5805;left:108"><nobr>five steps, and within each step, rules are applied</nobr></div>
<div style="position:absolute;top:5822;left:108"><nobr>until one of them passes the conditions. If a rule is</nobr></div>
<div style="position:absolute;top:5839;left:108"><nobr>accepted, the suffix is removed consequently, and the</nobr></div>
<div style="position:absolute;top:5856;left:108"><nobr>next step is performed. The resultant stem at the end</nobr></div>
<div style="position:absolute;top:5874;left:108"><nobr>of the fifth step is returned. </nobr></div>
<div style="position:absolute;top:5908;left:108"><nobr>The rule looks like the following: </nobr></div>
<div style="position:absolute;top:5923;left:108"><nobr><i><b>&lt;condition&gt; &lt;suffix&gt; → &lt;new suffix&gt;</b></i></nobr></div>
<div style="position:absolute;top:5957;left:108"><nobr>For example, a rule (m&gt;0) EED → EE means “if the</nobr></div>
<div style="position:absolute;top:5977;left:108"><nobr>word has at least one vowel and consonant plus EED</nobr></div>
<div style="position:absolute;top:5035;left:486"><nobr>ending, change the ending to EE”. So “agreed”</nobr></div>
<div style="position:absolute;top:5052;left:486"><nobr>becomes “agree” while “feed” remains unchanged.</nobr></div>
<div style="position:absolute;top:5073;left:486"><nobr>This algorithm has about 60 rules and very easy to</nobr></div>
<div style="position:absolute;top:5090;left:486"><nobr>understand. Porter designed a detailed framework of</nobr></div>
<div style="position:absolute;top:5104;left:486"><nobr>stemming which is known as „Snowball‟. The main</nobr></div>
<div style="position:absolute;top:5124;left:486"><nobr>purpose of the framework is to allow programmers to</nobr></div>
<div style="position:absolute;top:5142;left:486"><nobr>develop their own stemmers for other character sets</nobr></div>
<div style="position:absolute;top:5159;left:486"><nobr>or languages. </nobr></div>
<div style="position:absolute;top:5193;left:486"><nobr>However it was noted that Lovins stemmer is a</nobr></div>
<div style="position:absolute;top:5211;left:486"><nobr>heavier stemmer that produces a better data reduction</nobr></div>
<div style="position:absolute;top:5228;left:486"><nobr>[13]. The Lovins algorithm is obviously larger than</nobr></div>
<div style="position:absolute;top:5245;left:486"><nobr>the Porter algorithm, because of its very extensive</nobr></div>
<div style="position:absolute;top:5262;left:486"><nobr>endings list. But in one way that is used to advantage:</nobr></div>
<div style="position:absolute;top:5280;left:486"><nobr>it is faster. It has effectively traded space for time,</nobr></div>
<div style="position:absolute;top:5297;left:486"><nobr>and with its large suffix set it needs just two major</nobr></div>
<div style="position:absolute;top:5314;left:486"><nobr>steps to remove a suffix, compared with the five of</nobr></div>
<div style="position:absolute;top:5331;left:486"><nobr>the Porter algorithm.</nobr></div>
<div style="position:absolute;top:5366;left:486"><nobr><b>3. Paice/Husk Stemmer</b></nobr></div>
<div style="position:absolute;top:5383;left:486"><nobr>The Paice/Husk stemmer is an iterative algorithm</nobr></div>
<div style="position:absolute;top:5400;left:486"><nobr>with one table containing about 120 rules indexed by</nobr></div>
<div style="position:absolute;top:5418;left:486"><nobr>the last letter of a suffix [14].  It tries to find the</nobr></div>
<div style="position:absolute;top:5435;left:486"><nobr>applicable rule by the last character of the word. Each</nobr></div>
<div style="position:absolute;top:5452;left:486"><nobr>rule specifies either a deletion or replacement of an</nobr></div>
<div style="position:absolute;top:5470;left:486"><nobr>ending. If there is no such rule, it terminates. It also</nobr></div>
<div style="position:absolute;top:5487;left:486"><nobr>terminates if a word starts with a vowel and there are</nobr></div>
<div style="position:absolute;top:5504;left:486"><nobr>only two letters left or only three characters left.</nobr></div>
<div style="position:absolute;top:5521;left:486"><nobr>Otherwise, the rule is applied and the process repeats.</nobr></div>
<div style="position:absolute;top:5539;left:486"><nobr>The advantage is simple and every iteration taking</nobr></div>
<div style="position:absolute;top:5556;left:486"><nobr>care of both deletion and replacement as per the rule</nobr></div>
<div style="position:absolute;top:5573;left:486"><nobr>applied. The disadvantage is it is very heavy</nobr></div>
<div style="position:absolute;top:5590;left:486"><nobr>algorithm and over stemming may occur.</nobr></div>
<div style="position:absolute;top:5625;left:486"><nobr><b>4. Dawson Stemmer</b></nobr></div>
<div style="position:absolute;top:5642;left:486"><nobr>This stemmer is an extension of the Lovins approach</nobr></div>
<div style="position:absolute;top:5659;left:486"><nobr>except that it covers much more complete list of</nobr></div>
<div style="position:absolute;top:5677;left:486"><nobr>about 1200 suffixes. Like Lovins, it is also a single</nobr></div>
<div style="position:absolute;top:5694;left:486"><nobr>pass stemmer and hence it is pretty fast. The suffixes</nobr></div>
<div style="position:absolute;top:5711;left:486"><nobr>are stored in the reversed order indexed by their</nobr></div>
<div style="position:absolute;top:5728;left:486"><nobr>length and last letter. In fact they are organized as a</nobr></div>
<div style="position:absolute;top:5746;left:486"><nobr>set of divided character trees for rapid access. The</nobr></div>
<div style="position:absolute;top:5763;left:486"><nobr>advantage is that it covers more suffixes than Lovins</nobr></div>
<div style="position:absolute;top:5780;left:486"><nobr>and is fast in execution. The disadvantage is it is very</nobr></div>
<div style="position:absolute;top:5797;left:486"><nobr>complex, and</nobr></div>
<div style="position:absolute;top:5797;left:607"><nobr>lacks a standard reusable</nobr></div>
<div style="position:absolute;top:5815;left:486"><nobr>implementation [8].</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:14px;font-family:Times">
<div style="position:absolute;top:5871;left:486"><nobr><b>B. Statistical Methods</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:5892;left:486"><nobr>These are the stemmers who are based on statistical</nobr></div>
<div style="position:absolute;top:5909;left:486"><nobr>analysis and techniques. Most of the methods remove</nobr></div>
<div style="position:absolute;top:5926;left:486"><nobr>the affixes, but after implementing some statistical</nobr></div>
<div style="position:absolute;top:5943;left:486"><nobr>procedure [8].</nobr></div>
</span></font>
<font size="3" color="#800000" face="Times"><span style="font-size:13px;font-family:Times;color:#800000">
<div style="position:absolute;top:4963;left:82"><nobr><i><b>Dr.S.Vijayarani et al , International Journal of Computer Science &amp; Communication Networks,Vol 5(1),7-16</b></i></nobr></div>
<div style="position:absolute;top:6059;left:790"><nobr><i><b>11</b></i></nobr></div>
</span></font>
<font size="3" color="#ab4000" face="Times"><span style="font-size:13px;font-family:Times;color:#ab4000">
<div style="position:absolute;top:4941;left:763"><nobr><b>ISSN:2249-5789</b></nobr></div>
</span></font>

<div style="position:absolute;top:6115;left:0"><hr><table border="0" width="100%"><tbody><tr><td bgcolor="eeeeee" align="right"><font face="arial,sans-serif"><a name="6"><b>Page 6</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:6227;left:108"><nobr><b>1. N-Gram Stemmer</b></nobr></div>
<div style="position:absolute;top:6243;left:108"><nobr>It is language independent stemmer. The string-</nobr></div>
<div style="position:absolute;top:6261;left:108"><nobr>similarity approach is used to convert word inflation</nobr></div>
<div style="position:absolute;top:6278;left:108"><nobr>to its stem. N-gram is a string of n, usually adjacent,</nobr></div>
<div style="position:absolute;top:6295;left:108"><nobr>characters extracted from a section of continuous</nobr></div>
<div style="position:absolute;top:6312;left:108"><nobr>text. N-gram is a set of n following characters</nobr></div>
<div style="position:absolute;top:6330;left:108"><nobr>extracted from a word. The main idea behind this</nobr></div>
<div style="position:absolute;top:6347;left:108"><nobr>approach is, similar words will have a high quantity</nobr></div>
<div style="position:absolute;top:6364;left:108"><nobr>of n-grams in common. For n equals to 2 or 3, the</nobr></div>
<div style="position:absolute;top:6381;left:108"><nobr>words extracted are called diagrams or trigrams,</nobr></div>
<div style="position:absolute;top:6399;left:108"><nobr>respectively [7, 8].</nobr></div>
<div style="position:absolute;top:6433;left:112"><nobr>For example, the word </nobr></div>
<div style="position:absolute;top:6447;left:108"><nobr>„INTRODUCTIONS‟ results in the generation of the</nobr></div>
<div style="position:absolute;top:6468;left:108"><nobr>diagrams:</nobr></div>
<div style="position:absolute;top:6502;left:108"><nobr>*I, IN, NT, TR, RO, OD, DU, UC, CT, TI, IO, ON,</nobr></div>
<div style="position:absolute;top:6519;left:108"><nobr>NS, S* and the trigrams:</nobr></div>
<div style="position:absolute;top:6537;left:108"><nobr>**I, *IN, INT, NTR, TRO, ROD, ODU, DUC, UCT,</nobr></div>
<div style="position:absolute;top:6554;left:108"><nobr>CTI, TIO, ION, ONS, NS*, S**</nobr></div>
<div style="position:absolute;top:6588;left:108"><nobr>Where '*' denotes a padding space. There are n+1</nobr></div>
<div style="position:absolute;top:6606;left:108"><nobr>such diagram and n+2 such trigrams in a word</nobr></div>
<div style="position:absolute;top:6623;left:108"><nobr>containing n characters. Most stemmers are language-</nobr></div>
<div style="position:absolute;top:6640;left:108"><nobr>specific. Usually a value of 4 or 5 is selected for n.</nobr></div>
<div style="position:absolute;top:6658;left:108"><nobr>After that a textual data or document is analyzed for</nobr></div>
<div style="position:absolute;top:6675;left:108"><nobr>all the n-grams. It is clear that a word root generally</nobr></div>
<div style="position:absolute;top:6692;left:108"><nobr>occurs less frequently than its morphological form.</nobr></div>
<div style="position:absolute;top:6709;left:108"><nobr>This means a word generally has an affix associated</nobr></div>
<div style="position:absolute;top:6727;left:108"><nobr>with it.</nobr></div>
<div style="position:absolute;top:6761;left:108"><nobr>This stemmer has an advantage that it is language</nobr></div>
<div style="position:absolute;top:6778;left:108"><nobr>independent and hence very useful in many</nobr></div>
<div style="position:absolute;top:6796;left:108"><nobr>applications. The disadvantage is it requires huge</nobr></div>
<div style="position:absolute;top:6813;left:108"><nobr>memory and storage for creating and storing the n</nobr></div>
<div style="position:absolute;top:6830;left:108"><nobr>grams and indexes and hence it is not a practical</nobr></div>
<div style="position:absolute;top:6847;left:108"><nobr>approach.</nobr></div>
<div style="position:absolute;top:6882;left:108"><nobr><b>2. HMM Stemmer</b></nobr></div>
<div style="position:absolute;top:6899;left:108"><nobr>This stemmer is based on the concept of the Hidden</nobr></div>
<div style="position:absolute;top:6916;left:108"><nobr>Markov Model (HMMs) which are finite-state</nobr></div>
<div style="position:absolute;top:6934;left:108"><nobr>automata where transitions between states are ruled</nobr></div>
<div style="position:absolute;top:6951;left:108"><nobr>by probability functions. At each transition, the new</nobr></div>
<div style="position:absolute;top:6968;left:108"><nobr>state emits a symbol with a given probability. This</nobr></div>
<div style="position:absolute;top:6985;left:108"><nobr>model was proposed by Melucci and Orio [15]. This</nobr></div>
<div style="position:absolute;top:7003;left:108"><nobr>method is based on unsupervised learning and does</nobr></div>
<div style="position:absolute;top:7020;left:108"><nobr>not need a prior linguistic knowledge of the dataset.</nobr></div>
<div style="position:absolute;top:7037;left:108"><nobr>In this method the probability of each path can be</nobr></div>
<div style="position:absolute;top:7054;left:108"><nobr>computed and the most probable path is found in the</nobr></div>
<div style="position:absolute;top:7072;left:108"><nobr>automata graph. In order to apply HMMs to</nobr></div>
<div style="position:absolute;top:7089;left:108"><nobr>stemming, a sequence of letters that forms a word can</nobr></div>
<div style="position:absolute;top:7106;left:108"><nobr>be considered the result of a concatenation of two</nobr></div>
<div style="position:absolute;top:7124;left:108"><nobr>subsequences: a prefix and a suffix. A way to model</nobr></div>
<div style="position:absolute;top:7141;left:108"><nobr>this process is through an HMM where the states are</nobr></div>
<div style="position:absolute;top:7158;left:108"><nobr>divided into two disjoint sets: initial can be the stems</nobr></div>
<div style="position:absolute;top:7175;left:108"><nobr>only and the latter can be the stems or suffixes.</nobr></div>
<div style="position:absolute;top:6226;left:486"><nobr>Transitions between states define word structure</nobr></div>
<div style="position:absolute;top:6243;left:486"><nobr>process. There are some assumptions that can be</nobr></div>
<div style="position:absolute;top:6261;left:486"><nobr>made with this method:</nobr></div>
<div style="position:absolute;top:6278;left:507"><nobr>1. Initial states belong only to the stem-set – a</nobr></div>
<div style="position:absolute;top:6295;left:529"><nobr>word always starts with a stem</nobr></div>
<div style="position:absolute;top:6312;left:507"><nobr>2. Transitions from states of the suffix-set of</nobr></div>
<div style="position:absolute;top:6330;left:529"><nobr>states of the stem-set always have a null  </nobr></div>
<div style="position:absolute;top:6347;left:529"><nobr>probability - a word can be only a</nobr></div>
<div style="position:absolute;top:6364;left:529"><nobr>concatenation of a stem and a suffix.</nobr></div>
<div style="position:absolute;top:6381;left:507"><nobr>3. Final states belong to both sets - a stem can</nobr></div>
<div style="position:absolute;top:6399;left:529"><nobr>have a number of different derivations, but it</nobr></div>
<div style="position:absolute;top:6416;left:529"><nobr>may also have no suffix.</nobr></div>
<div style="position:absolute;top:6433;left:486"><nobr>The advantage of this method is it is unsupervised</nobr></div>
<div style="position:absolute;top:6450;left:486"><nobr>and hence knowledge of the language is not required.</nobr></div>
<div style="position:absolute;top:6468;left:486"><nobr>The disadvantage is it is a little complex and may</nobr></div>
<div style="position:absolute;top:6485;left:486"><nobr>over stem the words sometimes [15].</nobr></div>
<div style="position:absolute;top:6526;left:486"><nobr><b>3. YASS Stemmer</b></nobr></div>
<div style="position:absolute;top:6543;left:486"><nobr>The name is an acronym for Yet another Suffix</nobr></div>
<div style="position:absolute;top:6561;left:486"><nobr>Striper. This stemmer was proposed by Prasenjit</nobr></div>
<div style="position:absolute;top:6578;left:486"><nobr>Majumder [16]. According to the authors the</nobr></div>
<div style="position:absolute;top:6595;left:486"><nobr>performance of a stemmer generated by clustering a</nobr></div>
<div style="position:absolute;top:6612;left:486"><nobr>lexicon without any linguistic input is equivalent to</nobr></div>
<div style="position:absolute;top:6630;left:486"><nobr>that obtained using standard, rule-based stemmers</nobr></div>
<div style="position:absolute;top:6647;left:486"><nobr>such as Porter‟s. This stemmer comes under the class</nobr></div>
<div style="position:absolute;top:6664;left:486"><nobr>of statistical as well as corpus based. It does not rely</nobr></div>
<div style="position:absolute;top:6682;left:486"><nobr>on linguistic expertise. Retrieval experiments by the</nobr></div>
<div style="position:absolute;top:6699;left:486"><nobr>authors in French, English, and Bengali datasets</nobr></div>
<div style="position:absolute;top:6716;left:486"><nobr>which shows that the proposed approach is effective</nobr></div>
<div style="position:absolute;top:6733;left:486"><nobr>for languages that are primarily suffixed in nature.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:14px;font-family:Times">
<div style="position:absolute;top:6768;left:486"><nobr><b>C. Mixed Methods</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:6787;left:486"><nobr><b>1. Inflectional and Derivational Methods</b></nobr></div>
<div style="position:absolute;top:6804;left:486"><nobr>This is another approach in stemming and it involves</nobr></div>
<div style="position:absolute;top:6821;left:486"><nobr>both the inflectional as well as the derivational</nobr></div>
<div style="position:absolute;top:6838;left:486"><nobr>morphology analysis. The corpus should be very</nobr></div>
<div style="position:absolute;top:6856;left:486"><nobr>large to develop these types of stemmers and hence</nobr></div>
<div style="position:absolute;top:6873;left:486"><nobr>they are part of corpus base stemmers too. In case of</nobr></div>
<div style="position:absolute;top:6890;left:486"><nobr>inflectional the word variants are related to the</nobr></div>
<div style="position:absolute;top:6907;left:486"><nobr>language specific syntactic variations like a plural,</nobr></div>
<div style="position:absolute;top:6925;left:486"><nobr>gender, case, etc., whereas in derivational the word</nobr></div>
<div style="position:absolute;top:6942;left:486"><nobr>variants are related to the part-of-speech (POS) of a</nobr></div>
<div style="position:absolute;top:6959;left:486"><nobr>sentence where the word occurs [7].</nobr></div>
<div style="position:absolute;top:6994;left:486"><nobr><b>a. Krovetz Stemmer (KSTEM)</b></nobr></div>
<div style="position:absolute;top:7011;left:486"><nobr>The Krovetz stemmer was presented in 1993 by</nobr></div>
<div style="position:absolute;top:7028;left:486"><nobr>Robert Krovetz [17] and is a linguistic lexical</nobr></div>
<div style="position:absolute;top:7045;left:486"><nobr>validation stemmer. Since it is based on the</nobr></div>
<div style="position:absolute;top:7063;left:486"><nobr>inflectional property of words and the language</nobr></div>
<div style="position:absolute;top:7080;left:486"><nobr>syntax, it is very complicated in nature. It effectively</nobr></div>
<div style="position:absolute;top:7097;left:486"><nobr>and accurately removes inflectional suffixes in three</nobr></div>
<div style="position:absolute;top:7115;left:486"><nobr>steps:</nobr></div>
<div style="position:absolute;top:7132;left:540"><nobr>1. Transforming the plurals of a word to its</nobr></div>
<div style="position:absolute;top:7151;left:486"><nobr>singular form</nobr></div>
</span></font>
<font size="3" color="#800000" face="Times"><span style="font-size:13px;font-family:Times;color:#800000">
<div style="position:absolute;top:6151;left:82"><nobr><i><b>Dr.S.Vijayarani et al , International Journal of Computer Science &amp; Communication Networks,Vol 5(1),7-16</b></i></nobr></div>
<div style="position:absolute;top:7247;left:790"><nobr><i><b>12</b></i></nobr></div>
</span></font>
<font size="3" color="#ab4000" face="Times"><span style="font-size:13px;font-family:Times;color:#ab4000">
<div style="position:absolute;top:6129;left:763"><nobr><b>ISSN:2249-5789</b></nobr></div>
</span></font>

<div style="position:absolute;top:7303;left:0"><hr><table border="0" width="100%"><tbody><tr><td bgcolor="eeeeee" align="right"><font face="arial,sans-serif"><a name="7"><b>Page 7</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:7414;left:162"><nobr>2. Converting the past tense of a word to its</nobr></div>
<div style="position:absolute;top:7431;left:108"><nobr>present tense</nobr></div>
<div style="position:absolute;top:7446;left:162"><nobr>3. Removing the suffix „ing‟</nobr></div>
<div style="position:absolute;top:7466;left:108"><nobr>The conversion process first removes the suffix and</nobr></div>
<div style="position:absolute;top:7483;left:108"><nobr>then through the process of checking in a dictionary</nobr></div>
<div style="position:absolute;top:7500;left:108"><nobr>for any recoding, returns the stem to a word. The</nobr></div>
<div style="position:absolute;top:7518;left:108"><nobr>dictionary lookup also performs any transformations</nobr></div>
<div style="position:absolute;top:7535;left:108"><nobr>that are required due to spelling exception and also</nobr></div>
<div style="position:absolute;top:7552;left:108"><nobr>converts any stem produced into a real word. Since</nobr></div>
<div style="position:absolute;top:7569;left:108"><nobr>this stemmer does not find the stems for all word</nobr></div>
<div style="position:absolute;top:7587;left:108"><nobr>variants, it can be used as a pre stemmer before</nobr></div>
<div style="position:absolute;top:7604;left:108"><nobr>actually applying a stemming algorithm. This would</nobr></div>
<div style="position:absolute;top:7621;left:108"><nobr>increase the speed and effectiveness of the main</nobr></div>
<div style="position:absolute;top:7638;left:108"><nobr>stemmer. Compared to Porter and Paice / Husk, this</nobr></div>
<div style="position:absolute;top:7656;left:108"><nobr>is a very light stemmer. </nobr></div>
<div style="position:absolute;top:7690;left:108"><nobr>The Krovetz stemmer attempts to increase accuracy</nobr></div>
<div style="position:absolute;top:7707;left:108"><nobr>and robustness by treating spelling errors and</nobr></div>
<div style="position:absolute;top:7725;left:108"><nobr>meaningless stems. If the input document size is large</nobr></div>
<div style="position:absolute;top:7742;left:108"><nobr>this stemmer becomes weak and does not perform</nobr></div>
<div style="position:absolute;top:7759;left:108"><nobr>very effectively. The major and clear flaw in</nobr></div>
<div style="position:absolute;top:7776;left:108"><nobr>dictionary-based algorithms is their incapability to</nobr></div>
<div style="position:absolute;top:7794;left:108"><nobr>manage with words, which are not in the lexicon.</nobr></div>
<div style="position:absolute;top:7811;left:108"><nobr>This stemmer does not consistently produce a good</nobr></div>
<div style="position:absolute;top:7828;left:108"><nobr>recall and precision performance [17].</nobr></div>
<div style="position:absolute;top:7863;left:108"><nobr><b>b. Xerox Inflectional and Derivational Analyzer</b></nobr></div>
<div style="position:absolute;top:7880;left:108"><nobr>The linguistics groups at Xerox have developed a</nobr></div>
<div style="position:absolute;top:7897;left:108"><nobr>number of linguistic tools for English which can be</nobr></div>
<div style="position:absolute;top:7915;left:108"><nobr>used in information retrieval. In particular, they have</nobr></div>
<div style="position:absolute;top:7932;left:108"><nobr>produced an English lexical database which provides</nobr></div>
<div style="position:absolute;top:7949;left:108"><nobr>a morphological analysis of any word in the lexicon</nobr></div>
<div style="position:absolute;top:7966;left:108"><nobr>and identifies the base form. Xerox linguists have</nobr></div>
<div style="position:absolute;top:7984;left:108"><nobr>developed a lexical database for English and some</nobr></div>
<div style="position:absolute;top:8001;left:108"><nobr>other languages also which can analyze and generate</nobr></div>
<div style="position:absolute;top:8018;left:108"><nobr>inflectional and derivational morphology. The</nobr></div>
<div style="position:absolute;top:8035;left:108"><nobr>inflectional database reduces each surface word to the</nobr></div>
<div style="position:absolute;top:8053;left:108"><nobr>form which can be found in the dictionary, as follows</nobr></div>
<div style="position:absolute;top:8070;left:108"><nobr>[12]:</nobr></div>
</span></font>
<font size="3" face="Symbol"><span style="font-size:13px;font-family:Symbol">
<div style="position:absolute;top:8083;left:135"><nobr>•</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:8088;left:162"><nobr>nouns singular (e.g. children child)</nobr></div>
</span></font>
<font size="3" face="Symbol"><span style="font-size:13px;font-family:Symbol">
<div style="position:absolute;top:8102;left:135"><nobr>•</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:8107;left:162"><nobr>verbs infinitive (e.g. understood understand)</nobr></div>
</span></font>
<font size="3" face="Symbol"><span style="font-size:13px;font-family:Symbol">
<div style="position:absolute;top:8120;left:135"><nobr>•</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:8125;left:162"><nobr>adjectives positive form (e.g. best good)</nobr></div>
</span></font>
<font size="3" face="Symbol"><span style="font-size:13px;font-family:Symbol">
<div style="position:absolute;top:8139;left:135"><nobr>•</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:8143;left:162"><nobr>pronoun nominative (e.g. whom who)</nobr></div>
<div style="position:absolute;top:8161;left:108"><nobr>The derivational database reduces surface forms to</nobr></div>
<div style="position:absolute;top:8178;left:108"><nobr>stems which are related to the original in both form</nobr></div>
<div style="position:absolute;top:8195;left:108"><nobr>and semantics.</nobr></div>
<div style="position:absolute;top:8230;left:108"><nobr><b>Advantages</b></nobr></div>
<div style="position:absolute;top:8247;left:108"><nobr>This stemmer works well with a large document also</nobr></div>
<div style="position:absolute;top:8264;left:108"><nobr>and removes the prefixes also whereever applicable.</nobr></div>
<div style="position:absolute;top:8281;left:108"><nobr>All stems are valid words since a lexical database</nobr></div>
<div style="position:absolute;top:8299;left:108"><nobr>which provides a morphological analysis of any word</nobr></div>
<div style="position:absolute;top:8316;left:108"><nobr>in the lexicon is available for stemming. It has proved</nobr></div>
<div style="position:absolute;top:8333;left:108"><nobr>to work better than the Krovetz stemmer for a large</nobr></div>
<div style="position:absolute;top:8350;left:108"><nobr>corpus.</nobr></div>
<div style="position:absolute;top:7415;left:486"><nobr><b>Disadvantage</b></nobr></div>
<div style="position:absolute;top:7431;left:486"><nobr>The output depends on the lexical database which</nobr></div>
<div style="position:absolute;top:7449;left:486"><nobr>may not be exhaustive. Since this method is based on</nobr></div>
<div style="position:absolute;top:7466;left:486"><nobr>a lexicon, it cannot correctly stem the words which</nobr></div>
<div style="position:absolute;top:7483;left:486"><nobr>are not part of the lexicon. This stemmer has not been</nobr></div>
<div style="position:absolute;top:7500;left:486"><nobr>implemented successfully in many other languages.</nobr></div>
<div style="position:absolute;top:7518;left:486"><nobr>Dependence on the lexicon makes it a language</nobr></div>
<div style="position:absolute;top:7535;left:486"><nobr>dependent stemmer.</nobr></div>
<div style="position:absolute;top:7570;left:486"><nobr><b>2. Corpus Based Stemmer</b></nobr></div>
<div style="position:absolute;top:7587;left:486"><nobr>This method of stemming was proposed by Xu and</nobr></div>
<div style="position:absolute;top:7601;left:486"><nobr>Croft in their paper “Corpus-based stemming using</nobr></div>
<div style="position:absolute;top:7621;left:486"><nobr>co-occurrence of word variants” [8]. They have</nobr></div>
<div style="position:absolute;top:7638;left:486"><nobr>optional an approach which tries to overcome some</nobr></div>
<div style="position:absolute;top:7656;left:486"><nobr>of the drawbacks of Porter stemmer.</nobr></div>
<div style="position:absolute;top:7690;left:490"><nobr>For example, the words „policy‟ and „police‟ are</nobr></div>
<div style="position:absolute;top:7707;left:486"><nobr>conflated though they have a different meaning, but</nobr></div>
<div style="position:absolute;top:7725;left:486"><nobr>the word „index‟ and „indices‟ are not conflated</nobr></div>
<div style="position:absolute;top:7742;left:486"><nobr>though they have the same root. Porter stemmer also</nobr></div>
<div style="position:absolute;top:7759;left:486"><nobr>generates stems which are not real words like</nobr></div>
<div style="position:absolute;top:7773;left:486"><nobr>„iteration‟ becomes „iter‟ and „general‟ becomes</nobr></div>
<div style="position:absolute;top:7791;left:486"><nobr>„gener‟.</nobr></div>
<div style="position:absolute;top:7828;left:490"><nobr>Corpus based stemming refers to automatic</nobr></div>
<div style="position:absolute;top:7846;left:486"><nobr>modification of conflation classes – words that have</nobr></div>
<div style="position:absolute;top:7863;left:486"><nobr>resulted in a common stem, to suit the characteristics</nobr></div>
<div style="position:absolute;top:7880;left:486"><nobr>of a given text corpus using statistical methods. The</nobr></div>
<div style="position:absolute;top:7897;left:486"><nobr>advantage of this method is it can potentially avoid</nobr></div>
<div style="position:absolute;top:7915;left:486"><nobr>making conflations that are not appropriate for a</nobr></div>
<div style="position:absolute;top:7932;left:486"><nobr>given corpus and the result is an actual word and not</nobr></div>
<div style="position:absolute;top:7949;left:486"><nobr>an incomplete stem. The disadvantage is that we need</nobr></div>
<div style="position:absolute;top:7966;left:486"><nobr>to develop the statistical measure for every corpus</nobr></div>
<div style="position:absolute;top:7984;left:486"><nobr>separately and the processing time increases as in the</nobr></div>
<div style="position:absolute;top:8001;left:486"><nobr>first step two stemming algorithms are first used</nobr></div>
<div style="position:absolute;top:8018;left:486"><nobr>before using this method.</nobr></div>
<div style="position:absolute;top:8053;left:486"><nobr><b>3. Context Sensitive Stemmer</b></nobr></div>
<div style="position:absolute;top:8070;left:486"><nobr>This is a very interesting method of stemming unlike</nobr></div>
<div style="position:absolute;top:8087;left:486"><nobr>the usual method where stemming is done before</nobr></div>
<div style="position:absolute;top:8104;left:486"><nobr>indexing a document, over here for a Web Search,</nobr></div>
<div style="position:absolute;top:8122;left:486"><nobr>context sensitive analysis is done using statistical</nobr></div>
<div style="position:absolute;top:8139;left:486"><nobr>modeling on the query side. This method was</nobr></div>
<div style="position:absolute;top:8156;left:486"><nobr>proposed by Funchun Peng et. al.[19].</nobr></div>
<div style="position:absolute;top:8173;left:486"><nobr>Basically for the words of the input query, the</nobr></div>
<div style="position:absolute;top:8191;left:486"><nobr>morphological variants which would be useful for the</nobr></div>
<div style="position:absolute;top:8208;left:486"><nobr>search are predicted before the query is submitted to</nobr></div>
<div style="position:absolute;top:8225;left:486"><nobr>the search engine. This severely reduces the number</nobr></div>
<div style="position:absolute;top:8242;left:486"><nobr>of bad expansions, which in turn reduces the cost of</nobr></div>
<div style="position:absolute;top:8260;left:486"><nobr>additional computation and improves the precision at</nobr></div>
<div style="position:absolute;top:8277;left:486"><nobr>the same time. After the predicted word variants of</nobr></div>
<div style="position:absolute;top:8294;left:486"><nobr>the query have been derived, a context sensitive</nobr></div>
<div style="position:absolute;top:8312;left:486"><nobr>document matching is done for these variants. This</nobr></div>
<div style="position:absolute;top:8329;left:486"><nobr>conservative strategy serves as a safeguard against</nobr></div>
<div style="position:absolute;top:8346;left:486"><nobr>spurious stemming, and it turns out to be very</nobr></div>
<div style="position:absolute;top:8363;left:486"><nobr>important for improving precision. This stemming</nobr></div>
</span></font>
<font size="3" color="#800000" face="Times"><span style="font-size:13px;font-family:Times;color:#800000">
<div style="position:absolute;top:7339;left:82"><nobr><i><b>Dr.S.Vijayarani et al , International Journal of Computer Science &amp; Communication Networks,Vol 5(1),7-16</b></i></nobr></div>
<div style="position:absolute;top:8435;left:790"><nobr><i><b>13</b></i></nobr></div>
</span></font>
<font size="3" color="#ab4000" face="Times"><span style="font-size:13px;font-family:Times;color:#ab4000">
<div style="position:absolute;top:7317;left:763"><nobr><b>ISSN:2249-5789</b></nobr></div>
</span></font>

<div style="position:absolute;top:8491;left:0"><hr><table border="0" width="100%"><tbody><tr><td bgcolor="eeeeee" align="right"><font face="arial,sans-serif"><a name="8"><b>Page 8</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:8602;left:108"><nobr>process is divided into four steps [19] after the query</nobr></div>
<div style="position:absolute;top:8619;left:108"><nobr>is fired:</nobr></div>
<div style="position:absolute;top:8637;left:108"><nobr><b>a. Candidate generation:</b></nobr></div>
<div style="position:absolute;top:8654;left:108"><nobr>Over here the Porter stemmer is used to generate the</nobr></div>
<div style="position:absolute;top:8671;left:108"><nobr>stems from the query words. This has completely no</nobr></div>
<div style="position:absolute;top:8688;left:108"><nobr>relation to the semantics of the words. For a better</nobr></div>
<div style="position:absolute;top:8706;left:108"><nobr>output the corpus-based analysis based on</nobr></div>
<div style="position:absolute;top:8723;left:108"><nobr>distributional similarity is used. The foundation of</nobr></div>
<div style="position:absolute;top:8740;left:108"><nobr>using distributional word similarity is that true</nobr></div>
<div style="position:absolute;top:8757;left:108"><nobr>variants tend to be used in similar contexts. In the</nobr></div>
<div style="position:absolute;top:8775;left:108"><nobr>distributional word similarity calculation, each word</nobr></div>
<div style="position:absolute;top:8792;left:108"><nobr>is represented by a vector of features derived from</nobr></div>
<div style="position:absolute;top:8809;left:108"><nobr>the context of the word. We use the bigrams to the</nobr></div>
<div style="position:absolute;top:8826;left:108"><nobr>left and right of the word as its context features, by</nobr></div>
<div style="position:absolute;top:8844;left:108"><nobr>mining a huge Web corpus. The similarity between</nobr></div>
<div style="position:absolute;top:8861;left:108"><nobr>two words is the cosine similarity between the two</nobr></div>
<div style="position:absolute;top:8878;left:108"><nobr>corresponding feature vectors [7].</nobr></div>
<div style="position:absolute;top:8913;left:108"><nobr><b>b. Query Segmentation and head word detection:</b></nobr></div>
<div style="position:absolute;top:8930;left:108"><nobr>When the queries are long, it is important to detect</nobr></div>
<div style="position:absolute;top:8947;left:108"><nobr>the major concept of the query. The query is broken</nobr></div>
<div style="position:absolute;top:8964;left:108"><nobr>into segments which are normally the noun phrases.</nobr></div>
<div style="position:absolute;top:8982;left:108"><nobr>For each noun phrase the most important word is</nobr></div>
<div style="position:absolute;top:8999;left:108"><nobr>detected which is the head word. Sometimes a word</nobr></div>
<div style="position:absolute;top:9016;left:108"><nobr>is split to know the content. The mutual information</nobr></div>
<div style="position:absolute;top:9034;left:108"><nobr>of two adjacent words is found and if it passes a</nobr></div>
<div style="position:absolute;top:9051;left:108"><nobr>threshold value, they are kept in the same segment.</nobr></div>
<div style="position:absolute;top:9068;left:108"><nobr>Finding the headword is by using a syntactical parser</nobr></div>
<div style="position:absolute;top:9085;left:108"><nobr>[7, 8].</nobr></div>
<div style="position:absolute;top:9120;left:108"><nobr><b>c. Context sensitive word expansion:</b></nobr></div>
<div style="position:absolute;top:9137;left:108"><nobr>The keywords words are obtained by using</nobr></div>
<div style="position:absolute;top:9154;left:108"><nobr>probability measures and it decided which word</nobr></div>
<div style="position:absolute;top:9172;left:108"><nobr>variants would be most useful – generally they are</nobr></div>
<div style="position:absolute;top:9189;left:108"><nobr>the plural forms of the words. This is done using the</nobr></div>
<div style="position:absolute;top:9206;left:108"><nobr>simplest and most successful approach to language</nobr></div>
<div style="position:absolute;top:9223;left:108"><nobr>modeling, which is the one based on the <i>n</i>-gram</nobr></div>
<div style="position:absolute;top:9241;left:108"><nobr>model which uses the chain rule of probability. In</nobr></div>
<div style="position:absolute;top:9258;left:108"><nobr>this step all the important head word variants are</nobr></div>
<div style="position:absolute;top:9275;left:108"><nobr>obtained. The traditional way of using stemming for</nobr></div>
<div style="position:absolute;top:9292;left:108"><nobr>Web search, is referred as the naïve model. This is to</nobr></div>
<div style="position:absolute;top:9310;left:108"><nobr>treat every word variant equivalent for all possible</nobr></div>
<div style="position:absolute;top:9327;left:108"><nobr>words in the query. The query “book store” will be</nobr></div>
<div style="position:absolute;top:9341;left:108"><nobr>transformed into “<i>(book OR books) (store OR</i></nobr></div>
<div style="position:absolute;top:9361;left:108"><nobr><i>stores)</i>” when limiting stemming to pluralization</nobr></div>
<div style="position:absolute;top:9379;left:108"><nobr>handling only, where <i>OR </i>is an operator that denotes</nobr></div>
<div style="position:absolute;top:9396;left:108"><nobr>the equivalence of the left and right arguments [8].</nobr></div>
<div style="position:absolute;top:9431;left:108"><nobr><b>d. Context sensitive document matching:</b></nobr></div>
<div style="position:absolute;top:9448;left:108"><nobr>The context is the left or the right non-stop segments</nobr></div>
<div style="position:absolute;top:9465;left:108"><nobr>of the original word. Considering the fact that queries</nobr></div>
<div style="position:absolute;top:9482;left:108"><nobr>and documents may not represent the intent in</nobr></div>
<div style="position:absolute;top:9500;left:108"><nobr>exactly the same way, this proximity constraint is to</nobr></div>
<div style="position:absolute;top:9517;left:108"><nobr>allow variant occurrences within a window of some</nobr></div>
<div style="position:absolute;top:9534;left:108"><nobr>fixed size [7]. The smaller the window size is, the</nobr></div>
<div style="position:absolute;top:9551;left:108"><nobr>more restrictive the matching. The advantage of this</nobr></div>
<div style="position:absolute;top:8602;left:486"><nobr>stemmer is it improves selective word expansion on</nobr></div>
<div style="position:absolute;top:8619;left:486"><nobr>the query side and conservative word occurrence</nobr></div>
<div style="position:absolute;top:8637;left:486"><nobr>matching on the document side. The disadvantage is</nobr></div>
<div style="position:absolute;top:8654;left:486"><nobr>the processing time and the complex nature of the</nobr></div>
<div style="position:absolute;top:8671;left:486"><nobr>stemmer. There can be errors in finding the noun</nobr></div>
<div style="position:absolute;top:8688;left:486"><nobr>phrases in the query and the nearest words.</nobr></div>
<div style="position:absolute;top:8723;left:486"><nobr><b>Term Frequency-Inverse Document Frequency</b></nobr></div>
<div style="position:absolute;top:8740;left:486"><nobr>Term Frequency–Inverse Document Frequency (tf-</nobr></div>
<div style="position:absolute;top:8757;left:486"><nobr>idf) is a numerical statistic which reveals that a word</nobr></div>
<div style="position:absolute;top:8775;left:486"><nobr>is how important to a document in a collection. The</nobr></div>
<div style="position:absolute;top:8792;left:486"><nobr>Tf - IDF is often used as a weighting factor in</nobr></div>
<div style="position:absolute;top:8809;left:486"><nobr>information retrieval and text mining. The value of tf-</nobr></div>
<div style="position:absolute;top:8826;left:486"><nobr>idf increases proportionally to the number of times a</nobr></div>
<div style="position:absolute;top:8844;left:486"><nobr>word appears in the document, but is counteracting</nobr></div>
<div style="position:absolute;top:8861;left:486"><nobr>by the frequency of the word in the corpus. This can</nobr></div>
<div style="position:absolute;top:8878;left:486"><nobr>help to control the fact that some words are generally</nobr></div>
<div style="position:absolute;top:8895;left:486"><nobr>more common than others. Tf–IDF can be</nobr></div>
<div style="position:absolute;top:8913;left:486"><nobr>successfully used for stop-words filtering in various</nobr></div>
<div style="position:absolute;top:8930;left:486"><nobr>subject fields including text summarization and</nobr></div>
<div style="position:absolute;top:8947;left:486"><nobr>classification. Tf–IDF is the product of two statistics</nobr></div>
<div style="position:absolute;top:8964;left:486"><nobr>which are termed frequency and inverse document</nobr></div>
<div style="position:absolute;top:8982;left:486"><nobr>frequency. To further distinguish them, the number of</nobr></div>
<div style="position:absolute;top:8999;left:486"><nobr>times each term occurs in each document is counted</nobr></div>
<div style="position:absolute;top:9016;left:486"><nobr>and sums them all together. Term Frequency (TF) is</nobr></div>
<div style="position:absolute;top:9034;left:486"><nobr>defined as the number of times a term occurs in a</nobr></div>
<div style="position:absolute;top:9051;left:486"><nobr>document [20, 21].</nobr></div>
<div style="position:absolute;top:9104;left:486"><nobr>T/(t, d)=.5+</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size:8px;font-family:Times">
<div style="position:absolute;top:9099;left:642"><nobr>0.5∗/(t,d)</nobr></div>
<div style="position:absolute;top:9115;left:586"><nobr>Masimum occurences o/ words</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:9144;left:486"><nobr>Inverse Document Frequency<i>- </i>an Inverse Document</nobr></div>
<div style="position:absolute;top:9161;left:486"><nobr>Frequency (IDF) is a statistical weight used for</nobr></div>
<div style="position:absolute;top:9178;left:486"><nobr>measuring the importance of a term in a text</nobr></div>
<div style="position:absolute;top:9196;left:486"><nobr>document collection. IDF feature is incorporated</nobr></div>
<div style="position:absolute;top:9213;left:486"><nobr>which reduces the weight of terms that occur very</nobr></div>
<div style="position:absolute;top:9230;left:486"><nobr>frequently in the document set and increases the</nobr></div>
<div style="position:absolute;top:9247;left:486"><nobr>weight of terms that occur rarely.</nobr></div>
<div style="position:absolute;top:9288;left:495"><nobr>IDF (t, d)log =</nobr></div>
<div style="position:absolute;top:9277;left:692"><nobr>lDl</nobr></div>
<div style="position:absolute;top:9298;left:601"><nobr>(no. o/ doc. ,term t appearns)</nobr></div>
<div style="position:absolute;top:9364;left:486"><nobr>Then Term Frequency - Inverse document frequency</nobr></div>
<div style="position:absolute;top:9382;left:486"><nobr>[TF-IDF] is calculated for each word using the</nobr></div>
<div style="position:absolute;top:9399;left:486"><nobr>formula,</nobr></div>
<div style="position:absolute;top:9431;left:552"><nobr>Tfidf (t, f, d) = tf (t, d)*idf (t, d)</nobr></div>
<div style="position:absolute;top:9463;left:486"><nobr>In this equation (1) and (2) ft, d denotes the</nobr></div>
<div style="position:absolute;top:9481;left:486"><nobr>frequency of the occurrence of term t in document d. </nobr></div>
<div style="position:absolute;top:9498;left:486"><nobr>In equation (3) TF-IDF is calculated for each term in</nobr></div>
<div style="position:absolute;top:9515;left:486"><nobr>the document by using Term Frequency (Tft, d) and</nobr></div>
<div style="position:absolute;top:9532;left:486"><nobr>Inverse Document Frequency (idft, d).</nobr></div>
</span></font>
<font size="3" color="#800000" face="Times"><span style="font-size:13px;font-family:Times;color:#800000">
<div style="position:absolute;top:8527;left:82"><nobr><i><b>Dr.S.Vijayarani et al , International Journal of Computer Science &amp; Communication Networks,Vol 5(1),7-16</b></i></nobr></div>
<div style="position:absolute;top:9623;left:790"><nobr><i><b>14</b></i></nobr></div>
</span></font>
<font size="3" color="#ab4000" face="Times"><span style="font-size:13px;font-family:Times;color:#ab4000">
<div style="position:absolute;top:8505;left:763"><nobr><b>ISSN:2249-5789</b></nobr></div>
</span></font>

<div style="position:absolute;top:9679;left:0"><hr><table border="0" width="100%"><tbody><tr><td bgcolor="eeeeee" align="right"><font face="arial,sans-serif"><a name="9"><b>Page 9</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size:16px;font-family:Times">
<div style="position:absolute;top:9791;left:108"><nobr><b>4. Conclusion</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:9826;left:108"><nobr>Text mining is the process of seeking or extracting</nobr></div>
<div style="position:absolute;top:9843;left:108"><nobr>the useful information from the textual data. It tries to</nobr></div>
<div style="position:absolute;top:9861;left:108"><nobr>find interesting patterns from large databases.  It uses</nobr></div>
<div style="position:absolute;top:9878;left:108"><nobr>different pre-processing techniques likes stop words</nobr></div>
<div style="position:absolute;top:9895;left:108"><nobr>elimination and stemming. This paper has given</nobr></div>
<div style="position:absolute;top:9912;left:108"><nobr>complete information about the text mining</nobr></div>
<div style="position:absolute;top:9930;left:108"><nobr>preprocessing techniques, i.e. stop words elimination</nobr></div>
<div style="position:absolute;top:9947;left:108"><nobr>and stemming algorithms. We hope this paper will</nobr></div>
<div style="position:absolute;top:9961;left:108"><nobr>help the text mining researcher‟s community and they</nobr></div>
<div style="position:absolute;top:9981;left:108"><nobr>get good knowledge about various preprocessing</nobr></div>
<div style="position:absolute;top:9999;left:108"><nobr>techniques. </nobr></div>
<div style="position:absolute;top:10033;left:108"><nobr><b>Reference</b></nobr></div>
<div style="position:absolute;top:10050;left:108"><nobr>[1] Vishal Gupta and Gurpreet S. Lehal, A Survey of</nobr></div>
<div style="position:absolute;top:10068;left:108"><nobr>Text Mining Techniques and Applications,</nobr></div>
<div style="position:absolute;top:10085;left:108"><nobr>JOURNAL OF EMERGING TECHNOLOGIES IN</nobr></div>
<div style="position:absolute;top:10102;left:108"><nobr>WEB INTELLIGENCE, VOL. 1, NO. 1, AUGUST</nobr></div>
<div style="position:absolute;top:10119;left:108"><nobr>2009.</nobr></div>
<div style="position:absolute;top:10154;left:108"><nobr>[2] Shaidah Jusoh 1and Hejab M. Alfawareh,</nobr></div>
<div style="position:absolute;top:10171;left:108"><nobr>Techniques, Applications and Challenging Issue in</nobr></div>
<div style="position:absolute;top:10188;left:108"><nobr>Text Mining, IJCSI International Journal of</nobr></div>
<div style="position:absolute;top:10206;left:108"><nobr>Computer Science Issues, Vol. 9, Issue 6, No 2,</nobr></div>
<div style="position:absolute;top:10223;left:108"><nobr>November 2012, ISSN (Online): 1694-0814.</nobr></div>
<div style="position:absolute;top:10257;left:108"><nobr>[3] S.Jusoh and H.M. Alfawareh, Natural language</nobr></div>
<div style="position:absolute;top:10275;left:108"><nobr>interface for online sales, in Proceedings of  the </nobr></div>
<div style="position:absolute;top:10292;left:108"><nobr>International </nobr></div>
<div style="position:absolute;top:10309;left:108"><nobr>Conference  on  Intelligent  and  Advanced  System</nobr></div>
<div style="position:absolute;top:10326;left:108"><nobr>(ICIAS2007). Malaysia: IEEE, November 2007, pp.</nobr></div>
<div style="position:absolute;top:10344;left:108"><nobr>224–228.</nobr></div>
<div style="position:absolute;top:10378;left:108"><nobr>[4] Saleh Alsaleem, Automated Arabic Text</nobr></div>
<div style="position:absolute;top:10395;left:108"><nobr>Categorization Using SVM and NB, International</nobr></div>
<div style="position:absolute;top:10413;left:108"><nobr>Arab Journal of e-Technology, Vol. 2, No. 2, June</nobr></div>
<div style="position:absolute;top:10430;left:108"><nobr>2011.</nobr></div>
<div style="position:absolute;top:10464;left:108"><nobr>[5] M.F. Porter, An Algorithm for Suffix Stripping,</nobr></div>
<div style="position:absolute;top:10482;left:108"><nobr>Program, vol. 14, no. 3, pp. 130-137, 1980.</nobr></div>
<div style="position:absolute;top:10516;left:108"><nobr>[6] C.Ramasubramanian and R.Ramya, Effective Pre-</nobr></div>
<div style="position:absolute;top:10533;left:108"><nobr>Processing Activities in Text Mining using Improved</nobr></div>
<div style="position:absolute;top:10548;left:108"><nobr>Porter‟s Stemming Algorithm, International Journal</nobr></div>
<div style="position:absolute;top:10568;left:108"><nobr>of Advanced Research in Computer and</nobr></div>
<div style="position:absolute;top:10585;left:108"><nobr>Communication Engineering Vol. 2, Issue 12,</nobr></div>
<div style="position:absolute;top:10602;left:108"><nobr>December 2013, ISSN (Online) : 2278-1021.</nobr></div>
<div style="position:absolute;top:10637;left:108"><nobr>[7] Ms. Anjali Ganesh Jivani, A Comparative Study</nobr></div>
<div style="position:absolute;top:10654;left:108"><nobr>of Stemming Algorithms, Anjali Ganesh Jivani et al,</nobr></div>
<div style="position:absolute;top:10671;left:108"><nobr>Int. J. Comp. Tech. Appl., Vol 2 (6), 1930-1938,</nobr></div>
<div style="position:absolute;top:10689;left:108"><nobr>ISSN:2229-6093<b><font color="#800000">.</font></b></nobr></div>
<div style="position:absolute;top:10723;left:108"><nobr>[8] Deepika Sharma, Stemming Algorithms, A</nobr></div>
<div style="position:absolute;top:10740;left:108"><nobr>Comparative Study and their Analysis, International</nobr></div>
<div style="position:absolute;top:9790;left:486"><nobr>Journal of Applied Information Systems (IJAIS) –</nobr></div>
<div style="position:absolute;top:9807;left:486"><nobr>ISSN : 2249-0868,  Foundation of Computer Science</nobr></div>
<div style="position:absolute;top:9825;left:486"><nobr>FCS, New York, USA, Volume 4– No.3, September</nobr></div>
<div style="position:absolute;top:9842;left:486"><nobr>2012 –<a href="http://www.ijais.org/"> </a><font color="#0000ff"><a href="http://www.ijais.org/">www.ijais.org</a></font><a href="http://www.ijais.org/"></a>.</nobr></div>
<div style="position:absolute;top:9876;left:486"><nobr>[9] Harman Donna, How effective is suffixing? </nobr></div>
<div style="position:absolute;top:9895;left:486"><nobr>Journal of the American Society for Information</nobr></div>
<div style="position:absolute;top:9912;left:486"><nobr>Science, 1991; 42, 7-15 7.</nobr></div>
<div style="position:absolute;top:9947;left:486"><nobr>[10] J. B. Lovins, Development of a stemming</nobr></div>
<div style="position:absolute;top:9964;left:486"><nobr>algorithm, Mechanical Translation and Computer</nobr></div>
<div style="position:absolute;top:9981;left:486"><nobr>Linguistic., vol.11, no.1/2, pp. 22-31, 1968.</nobr></div>
<div style="position:absolute;top:10016;left:486"><nobr>[11] Porter M.F, An algorithm for suffix stripping,</nobr></div>
<div style="position:absolute;top:10033;left:486"><nobr>Program. 1980; 14, 130-137.</nobr></div>
<div style="position:absolute;top:10067;left:486"><nobr>[12] Porter M.F, Snowball: A language for stemming</nobr></div>
<div style="position:absolute;top:10085;left:486"><nobr>algorithms. 2001.</nobr></div>
<div style="position:absolute;top:10119;left:486"><nobr>[13] Mladenic Dunja, Automatic word</nobr></div>
<div style="position:absolute;top:10136;left:486"><nobr>lemmatization. Proceedings B of the 5th International</nobr></div>
<div style="position:absolute;top:10154;left:486"><nobr>Multi- Conference Information Society IS. 2002,</nobr></div>
<div style="position:absolute;top:10171;left:486"><nobr>153-159.</nobr></div>
<div style="position:absolute;top:10205;left:486"><nobr>[14] Paice Chris D, Another stemmer. ACM SIGIR</nobr></div>
<div style="position:absolute;top:10223;left:486"><nobr>Forum, Volume 24, No. 3. 1990, 56-61.</nobr></div>
<div style="position:absolute;top:10257;left:486"><nobr>[15] Melucci Massimo and Orio Nicola, A novel</nobr></div>
<div style="position:absolute;top:10274;left:486"><nobr>method for stemmer generation based on hidden</nobr></div>
<div style="position:absolute;top:10292;left:486"><nobr>Markov models. Proceedings of the twelfth</nobr></div>
<div style="position:absolute;top:10309;left:486"><nobr>international conference on Information and</nobr></div>
<div style="position:absolute;top:10326;left:486"><nobr>knowledge management. 2003, 131-138.</nobr></div>
<div style="position:absolute;top:10361;left:486"><nobr>[16] Plisson Joel, Lavrac Nada and Mladenic Dunja,</nobr></div>
<div style="position:absolute;top:10378;left:486"><nobr>A rule based approach to word lemmatization.</nobr></div>
<div style="position:absolute;top:10395;left:486"><nobr>Proceedings C of the 7th International Multi-</nobr></div>
<div style="position:absolute;top:10412;left:486"><nobr>Conference Information Society IS. 2004.</nobr></div>
<div style="position:absolute;top:10447;left:486"><nobr>[17] Krovetz Robert, Viewing morphology as an</nobr></div>
<div style="position:absolute;top:10464;left:486"><nobr>inference process. Proceedings of the 16th annual</nobr></div>
<div style="position:absolute;top:10481;left:486"><nobr>international ACM SIGIR conference on Research</nobr></div>
<div style="position:absolute;top:10499;left:486"><nobr>and development in information retrieval. 1993,191-</nobr></div>
<div style="position:absolute;top:10516;left:486"><nobr>202. </nobr></div>
<div style="position:absolute;top:10550;left:486"><nobr>[18] Xu Jinxi and Croft Bruce W, Corpus-based</nobr></div>
<div style="position:absolute;top:10568;left:486"><nobr>stemming using co-occurrence of word variants.</nobr></div>
<div style="position:absolute;top:10585;left:486"><nobr>ACM Transactions on Information Systems. Volume</nobr></div>
<div style="position:absolute;top:10602;left:486"><nobr>16, Issue 1. 1998, 61-81.</nobr></div>
<div style="position:absolute;top:10637;left:486"><nobr>[19] Funchun Peng, Nawaaz Ahmed, Xin Li and</nobr></div>
<div style="position:absolute;top:10654;left:486"><nobr>Yumao Lu, Context sensitive stemming for web</nobr></div>
<div style="position:absolute;top:10671;left:486"><nobr>search.</nobr></div>
<div style="position:absolute;top:10688;left:486"><nobr>Proceedings of the 30th annual international ACM</nobr></div>
<div style="position:absolute;top:10706;left:486"><nobr>SIGIR conference on Research and development in</nobr></div>
<div style="position:absolute;top:10723;left:486"><nobr>information retrieval. 2007, 639-646.</nobr></div>
</span></font>
<font size="3" color="#800000" face="Times"><span style="font-size:13px;font-family:Times;color:#800000">
<div style="position:absolute;top:9715;left:82"><nobr><i><b>Dr.S.Vijayarani et al , International Journal of Computer Science &amp; Communication Networks,Vol 5(1),7-16</b></i></nobr></div>
<div style="position:absolute;top:10811;left:790"><nobr><i><b>15</b></i></nobr></div>
</span></font>
<font size="3" color="#ab4000" face="Times"><span style="font-size:13px;font-family:Times;color:#ab4000">
<div style="position:absolute;top:9693;left:763"><nobr><b>ISSN:2249-5789</b></nobr></div>
</span></font>

<div style="position:absolute;top:10867;left:0"><hr><table border="0" width="100%"><tbody><tr><td bgcolor="eeeeee" align="right"><font face="arial,sans-serif"><a name="10"><b>Page 10</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size:13px;font-family:Times">
<div style="position:absolute;top:10978;left:108"><nobr>[20]  Menaka S and  Radha N,  Text Classification</nobr></div>
<div style="position:absolute;top:10995;left:108"><nobr>using Keyword Extraction Technique, International</nobr></div>
<div style="position:absolute;top:11013;left:108"><nobr>Journal of Advanced Research in  Computer Science</nobr></div>
<div style="position:absolute;top:11030;left:108"><nobr>and Software Engineering, Volume 3, Issue 12,</nobr></div>
<div style="position:absolute;top:11047;left:108"><nobr>December 2013, ISSN: 2277 128X.</nobr></div>
<div style="position:absolute;top:11082;left:108"><nobr>[21] S.Charanyaa and K.Sangeetha, Term Frequency</nobr></div>
<div style="position:absolute;top:11099;left:108"><nobr>Based Sequence Generation Algorithm for Graph</nobr></div>
<div style="position:absolute;top:11116;left:108"><nobr>Based Data Anonymization, International Journal of</nobr></div>
<div style="position:absolute;top:11133;left:108"><nobr>Innovative Research in Computer and</nobr></div>
<div style="position:absolute;top:11151;left:108"><nobr>Communication Engineering, (An ISO 3297: 2007</nobr></div>
<div style="position:absolute;top:11168;left:108"><nobr>Certified Organization), Vol. 2, Issue 2, February</nobr></div>
<div style="position:absolute;top:11185;left:108"><nobr>2014, ISSN(Online): 2320-9801.</nobr></div>
<div style="position:absolute;top:11220;left:108"><nobr>[22] Anjali Ganesh Jivani , A Comparative Study of</nobr></div>
<div style="position:absolute;top:11237;left:108"><nobr>Stemming Algorithms, International Journal  of</nobr></div>
<div style="position:absolute;top:11254;left:108"><nobr>Computer, Technology and Application, Volume 2,</nobr></div>
<div style="position:absolute;top:11271;left:108"><nobr>ISSN:2229-6093.</nobr></div>
<div style="position:absolute;top:11306;left:108"><nobr>[23] Vishal Gupta, Gurpreet Singh Lehal, A Survey</nobr></div>
<div style="position:absolute;top:11323;left:108"><nobr>of Common Stemming Techniques and Existing</nobr></div>
<div style="position:absolute;top:11340;left:108"><nobr>Stemmers for Indian Languages, Journal of Emerging </nobr></div>
<div style="position:absolute;top:11358;left:108"><nobr>Techniloigies in Web Intelligence, VOL. 5, NO. 2,</nobr></div>
<div style="position:absolute;top:11375;left:108"><nobr>MAY 2013.</nobr></div>
<div style="position:absolute;top:11410;left:108"><nobr>[24] Agbele, A.O. Adesina, N.A. Azeez, &amp; A.P.</nobr></div>
<div style="position:absolute;top:11427;left:108"><nobr>Abidoye , Context-Aware Stemming Algorithm for</nobr></div>
<div style="position:absolute;top:11444;left:108"><nobr>Semantically Related Root Words, African Journal of</nobr></div>
<div style="position:absolute;top:11461;left:108"><nobr>Computing &amp; ICT.</nobr></div>
<div style="position:absolute;top:11496;left:108"><nobr>[25] Hassan Saif,  Miriam Fernandez,Yulan He,</nobr></div>
<div style="position:absolute;top:11513;left:108"><nobr>Harith Alani , On Stopwords, Filtering and Data</nobr></div>
<div style="position:absolute;top:11530;left:108"><nobr>Sparsity for Sentiment Analysis of Twitter.</nobr></div>
</span></font>
<font size="3" color="#800000" face="Times"><span style="font-size:13px;font-family:Times;color:#800000">
<div style="position:absolute;top:10903;left:82"><nobr><i><b>Dr.S.Vijayarani et al , International Journal of Computer Science &amp; Communication Networks,Vol 5(1),7-16</b></i></nobr></div>
<div style="position:absolute;top:11999;left:790"><nobr><i><b>16</b></i></nobr></div>
</span></font>
<font size="3" color="#ab4000" face="Times"><span style="font-size:13px;font-family:Times;color:#ab4000">
<div style="position:absolute;top:10881;left:763"><nobr><b>ISSN:2249-5789</b></nobr></div>
</span></font>
<!-- t38278r97a38c33054e1822n31135u65l0m0k0 -->


</div></body></html>